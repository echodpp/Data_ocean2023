{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## MODIS (1080x2160)"
      ],
      "metadata": {
        "id": "IAp7O9WS_gay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "# Make sure 'data/' directory exists\n",
        "if not os.path.exists('data/'):\n",
        "    os.makedirs('data/')\n",
        "    \n",
        "\n",
        "# Download the data from MODIS (1080x2160):  http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.sst.modis.php\n",
        "\n",
        "modis_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/sst.modis.r2022/hdf/sst.m.{}.tar'\n",
        "\n",
        "for year in range(2002, 2023):  # 2023 is not included\n",
        "    url = modis_url.format(year)\n",
        "    filename = f'data/sst.m.{year}.tar'\n",
        "    download_file(url, filename)\n",
        "    print(f\"Downloaded: {filename}\")\n",
        "\n",
        "\n",
        "# Download the satellite data (1080x2160): http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.m.chl.m.sst.php\n",
        "\n",
        "#base_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/vgpm.r2022.m.chl.m.sst/hdf/vgpm.m.{}.tar'\n",
        "\n",
        "#for year in range(2002, 2023):  # 2023 is not included\n",
        "    #url = base_url.format(year)\n",
        "    #filename = f'data/vgpm.m.{year}.tar'\n",
        "    #download_file(url, filename)\n",
        "    #print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "id": "8RvXMDbHKtgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Mean"
      ],
      "metadata": {
        "id": "TmSnw-xY_9z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import tempfile\n",
        "!pip install pyhdf\n",
        "from pyhdf.SD import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the latitude and longitude arrays\n",
        "lats = np.linspace(90, -90, 1080)\n",
        "lons = np.linspace(-180, 180, 2160)\n",
        "\n",
        "# Create a meshgrid of latitudes and longitudes\n",
        "lon_grid, lat_grid = np.meshgrid(lons, lats, indexing='ij')\n",
        "\n",
        "# Flatten the arrays for dataframe construction\n",
        "lat_flat = lat_grid.flatten()\n",
        "lon_flat = lon_grid.flatten()\n",
        "\n",
        "# Create a multi-index from the latitude and longitude arrays\n",
        "index = pd.MultiIndex.from_arrays([lat_flat, lon_flat], names=['Lat', 'Long'])\n",
        "\n",
        "# Create an empty DataFrame with this index\n",
        "df = pd.DataFrame(index=index)\n",
        "\n",
        "# Loop through each tar file\n",
        "for year in range(2002, 2023):\n",
        "    # Open the tar file\n",
        "    tar_filename = f'data/sst.m.{year}.tar'\n",
        "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
        "        \n",
        "        # Create an empty DataFrame to store data for this year\n",
        "        temp_df = pd.DataFrame(index=index)\n",
        "\n",
        "        # Loop through each member of the tar file\n",
        "        for member in tar.getmembers():\n",
        "            # If it's a .hdf.gz file\n",
        "            if member.name.endswith('.hdf.gz'):\n",
        "                # Open the .hdf.gz file\n",
        "                f = tar.extractfile(member)\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    # Decompress the .hdf.gz file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile() as tmp:\n",
        "                        tmp.write(gz.read())\n",
        "                        tmp.seek(0)  # Go back to the start of the file\n",
        "\n",
        "                        # Open the temporary .hdf file\n",
        "                        hdf_file = SD(tmp.name, SDC.READ)\n",
        "\n",
        "                        # Access the 'sst' dataset\n",
        "                        data = hdf_file.select('sst')[:]\n",
        "\n",
        "                        # Replace '-9999.0' with NaN\n",
        "                        data[data == -9999.0] = np.nan\n",
        "\n",
        "                        # Flatten the data and add it to the temporary DataFrame\n",
        "                        data_flat = data.flatten()\n",
        "                        temp_df[member.name] = data_flat\n",
        "\n",
        "                        # Close the file\n",
        "                        hdf_file.end()\n",
        "\n",
        "        # After going through all files for the year, calculate the mean for each location (ignoring NaNs)\n",
        "        mean_data = temp_df.mean(axis=1, skipna=True)\n",
        "\n",
        "        # Add this DataFrame to the main DataFrame\n",
        "        df[str(year)] = mean_data\n",
        "\n",
        "# Reset the index of the DataFrame, making 'Lat' and 'Long' normal columns\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('sst_m_means.csv', index=False)"
      ],
      "metadata": {
        "id": "UDEZ8e5-mTNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sst_m_means = pd.read_csv('sst_m_means.csv')\n",
        "df_sst_m_means.head()"
      ],
      "metadata": {
        "id": "DY_e0hqtn7zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sst_m_means.describe()"
      ],
      "metadata": {
        "id": "ZSb_EtRCn_By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Cleaned Excel"
      ],
      "metadata": {
        "id": "1Cf3UV8DoEEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read new data\n",
        "# read each page from excel file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "filename='cleaned'\n",
        "Pacific=pd.read_excel(filename+'.xlsx',sheet_name='Pacific')\n",
        "Atlantic=pd.read_excel(filename+'.xlsx',sheet_name='Atlantic')\n",
        "Mediterranean=pd.read_excel(filename+'.xlsx',sheet_name='Mediterranean')\n",
        "Southern=pd.read_excel(filename+'.xlsx',sheet_name='Southern Ocean')\n",
        "Arctic=pd.read_excel(filename+'.xlsx',sheet_name='Arctic')\n"
      ],
      "metadata": {
        "id": "5R9g7-6bn8Xq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melt for Merge"
      ],
      "metadata": {
        "id": "ztjCZ_WxoJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melt the data\n",
        "df_sst_m_melt = df_sst_m_means.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='sst')"
      ],
      "metadata": {
        "id": "EuNTwOBQpPHi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select Decimal"
      ],
      "metadata": {
        "id": "avzvexwBpbMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Then merge on these columns\n",
        "# assuming df_melted_variable1-5 and ocean_dfs are your dataframes\n",
        "ocean_names = [\"Pacific\", \"Atlantic\", \"Mediterranean\", \"Southern\", \"Arctic\"]\n",
        "\n",
        "# change this to your dataframes\n",
        "df_melted_list = [df_sst_m_melt]\n",
        "ocean_dfs = [Pacific, Atlantic, Mediterranean, Southern, Arctic]\n",
        "\n",
        "\n",
        "for df in df_melted_list + ocean_dfs:\n",
        "    # convert types safely\n",
        "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
        "    df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
        "    df['Long'] = pd.to_numeric(df['Long'], errors='coerce')\n",
        "\n",
        "\n",
        "# round lat and long to 0.0001\n",
        "# I believe round is a way of thresholding the data, let me know if there is a better way of matching \n",
        "# the lat and longs\n",
        "for df in df_melted_list + ocean_dfs:\n",
        "    df['Lat'] = df['Lat'].round(1)\n",
        "    df['Long'] = df['Long'].round(1)"
      ],
      "metadata": {
        "id": "qRydS4-9pabv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there is a match\n",
        "# Create a set of tuples for the Pacific dataframe\n",
        "pacific_coords = set(zip(Pacific['Lat'], Pacific['Long']))\n",
        "\n",
        "# Create a set of tuples for the df_npp dataframe\n",
        "npp_coords = set(zip(df_sst_m_melt['Lat'], df_sst_m_melt['Long']))\n",
        "\n",
        "# Find the common coordinates\n",
        "common_coords = pacific_coords.intersection(npp_coords)\n",
        "\n",
        "print(f\"Common coordinates between Pacific and df_npp: {len(common_coords)}\")"
      ],
      "metadata": {
        "id": "WF27zIYSqAce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge"
      ],
      "metadata": {
        "id": "nZ1IwPYlqKCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "merged_dfs = []  # List to store merged dataframes\n",
        "\n",
        "for ocean_name, ocean_df in zip(ocean_names, ocean_dfs):\n",
        "    # merge ocean_df with all dataframes in df_melted_list\n",
        "    for df_melted in df_melted_list:\n",
        "        ocean_df = pd.merge(ocean_df, df_melted, on=['Lat', 'Long', 'Year'], how='left')\n",
        "\n",
        "    # Fill NaN values with the mean of each column\n",
        "    for col in ocean_df.select_dtypes(include=[np.number]).columns:\n",
        "        ocean_df[col] = ocean_df[col].astype(float)\n",
        "        ocean_df[col].fillna(ocean_df[col].mean(), inplace=True)\n",
        "        if ocean_df[col].apply(float.is_integer).all():  # Check if all values are integer\n",
        "            ocean_df[col] = ocean_df[col].astype('Int64')  # Change dtype back to integer\n",
        "\n",
        "    merged_dfs.append(ocean_df)  # Append the merged dataframe to the list\n",
        "\n",
        "with pd.ExcelWriter('merged_sst_m.xlsx') as writer:\n",
        "    for ocean_name, merged_df in zip(ocean_names, merged_dfs):\n",
        "        merged_df.to_excel(writer, sheet_name=ocean_name)  # Write each merged dataframe to a different sheet"
      ],
      "metadata": {
        "id": "XqyMtxMdqMGP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SeaWIFS/AVHRR (1024x2048)"
      ],
      "metadata": {
        "id": "LbOSHsVJX3oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "# Make sure 'data/' directory exists\n",
        "if not os.path.exists('data/'):\n",
        "    os.makedirs('data/')\n",
        "\n",
        "\n",
        "# Download the data from SeaWIFS/AVHRR (1024x2048): http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.sst.avhrr.php\n",
        "\n",
        "seawifs_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/sst.avhrr/hdf/sst.a.{}.tar'\n",
        "\n",
        "for year in range(1997, 2002):  # 2003 is not included\n",
        "    url = seawifs_url.format(year)\n",
        "    filename = f'data/sst.a.{year}.tar'\n",
        "    download_file(url, filename)\n",
        "    print(f\"Downloaded: {filename}\")\n",
        "\n",
        "\n",
        "# Download satellite data (1080x2160): http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.s.chl.a.sst.php\n",
        "\n",
        "#new_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/vgpm.r2014.s.chl.a.sst/hdf/vgpm.s.{}.tar'\n",
        "\n",
        "#for year in range(1997, 2002):  # 2023 is not included\n",
        "    #url = new_url.format(year)\n",
        "    #filename = f'data/vgpm.s.{year}.tar'\n",
        "    #download_file(url, filename)\n",
        "    #print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "id": "wzE1SCPIIE_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Mean"
      ],
      "metadata": {
        "id": "wXmBmqYMq6RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import tempfile\n",
        "!pip install pyhdf\n",
        "from pyhdf.SD import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the latitude and longitude arrays\n",
        "lats = np.linspace(90, -90, 1024)\n",
        "lons = np.linspace(-180, 180, 2048)\n",
        "\n",
        "# Create a meshgrid of latitudes and longitudes\n",
        "lon_grid, lat_grid = np.meshgrid(lons, lats, indexing='ij')\n",
        "\n",
        "# Flatten the arrays for dataframe construction\n",
        "lat_flat = lat_grid.flatten()\n",
        "lon_flat = lon_grid.flatten()\n",
        "\n",
        "# Create a multi-index from the latitude and longitude arrays\n",
        "index = pd.MultiIndex.from_arrays([lat_flat, lon_flat], names=['Lat', 'Long'])\n",
        "\n",
        "# Create an empty DataFrame with this index\n",
        "df = pd.DataFrame(index=index)\n",
        "\n",
        "# Loop through each tar file\n",
        "for year in range(1997, 2002):\n",
        "    # Open the tar file\n",
        "    tar_filename = f'data/sst.a.{year}.tar'\n",
        "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
        "        \n",
        "        # Create an empty DataFrame to store data for this year\n",
        "        temp_df = pd.DataFrame(index=index)\n",
        "\n",
        "        # Loop through each member of the tar file\n",
        "        for member in tar.getmembers():\n",
        "            # If it's a .hdf.gz file\n",
        "            if member.name.endswith('.hdf.gz'):\n",
        "                # Open the .hdf.gz file\n",
        "                f = tar.extractfile(member)\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    # Decompress the .hdf.gz file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile() as tmp:\n",
        "                        tmp.write(gz.read())\n",
        "                        tmp.seek(0)  # Go back to the start of the file\n",
        "\n",
        "                        # Open the temporary .hdf file\n",
        "                        hdf_file = SD(tmp.name, SDC.READ)\n",
        "\n",
        "                        # Access the 'sst' dataset\n",
        "                        data = hdf_file.select('sst')[:]\n",
        "\n",
        "                        # Replace '-9999.0' with NaN\n",
        "                        data[data == -9999.0] = np.nan\n",
        "\n",
        "                        # Flatten the data and add it to the temporary DataFrame\n",
        "                        data_flat = data.flatten()\n",
        "                        temp_df[member.name] = data_flat\n",
        "\n",
        "                        # Close the file\n",
        "                        hdf_file.end()\n",
        "\n",
        "        # After going through all files for the year, calculate the mean for each location (ignoring NaNs)\n",
        "        mean_data = temp_df.mean(axis=1, skipna=True)\n",
        "\n",
        "        # Add this DataFrame to the main DataFrame\n",
        "        df[str(year)] = mean_data\n",
        "\n",
        "# Reset the index of the DataFrame, making 'Lat' and 'Long' normal columns\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('sst_a_means.csv', index=False)"
      ],
      "metadata": {
        "id": "eXv05igCq19j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sst_a_means = pd.read_csv('sst_a_means.csv')\n",
        "df_sst_a_means.head()"
      ],
      "metadata": {
        "id": "LPQFm4nIthgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sst_a_means.describe()"
      ],
      "metadata": {
        "id": "mDWV-ZXStjS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Cleaned Excel"
      ],
      "metadata": {
        "id": "0r2IFW_dtn80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read new data\n",
        "# read each page from excel file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "filename='cleaned'\n",
        "Pacific=pd.read_excel(filename+'.xlsx',sheet_name='Pacific')\n",
        "Atlantic=pd.read_excel(filename+'.xlsx',sheet_name='Atlantic')\n",
        "Mediterranean=pd.read_excel(filename+'.xlsx',sheet_name='Mediterranean')\n",
        "Southern=pd.read_excel(filename+'.xlsx',sheet_name='Southern Ocean')\n",
        "Arctic=pd.read_excel(filename+'.xlsx',sheet_name='Arctic')"
      ],
      "metadata": {
        "id": "Aqa5nWFitpgR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melt for Merge"
      ],
      "metadata": {
        "id": "RSqxbeFrts0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melt the data\n",
        "df_sst_a_melt = df_sst_a_means.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='sst')"
      ],
      "metadata": {
        "id": "4eeoKmI-tqTr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select Decimal"
      ],
      "metadata": {
        "id": "9b9NdfL5tyaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Then merge on these columns\n",
        "# assuming df_melted_variable1-5 and ocean_dfs are your dataframes\n",
        "ocean_names = [\"Pacific\", \"Atlantic\", \"Mediterranean\", \"Southern\", \"Arctic\"]\n",
        "\n",
        "# change this to your dataframes\n",
        "df_melted_list = [df_sst_a_melt]\n",
        "ocean_dfs = [Pacific, Atlantic, Mediterranean, Southern, Arctic]\n",
        "\n",
        "\n",
        "for df in df_melted_list + ocean_dfs:\n",
        "    # convert types safely\n",
        "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
        "    df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
        "    df['Long'] = pd.to_numeric(df['Long'], errors='coerce')\n",
        "\n",
        "\n",
        "# round lat and long to 0.0001\n",
        "# I believe round is a way of thresholding the data, let me know if there is a better way of matching \n",
        "# the lat and longs\n",
        "for df in df_melted_list + ocean_dfs:\n",
        "    df['Lat'] = df['Lat'].round(1)\n",
        "    df['Long'] = df['Long'].round(1)"
      ],
      "metadata": {
        "id": "3skZHSbgtv0G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there is a match\n",
        "# Create a set of tuples for the Pacific dataframe\n",
        "pacific_coords = set(zip(Pacific['Lat'], Pacific['Long']))\n",
        "\n",
        "# Create a set of tuples for the df_npp dataframe\n",
        "npp_coords = set(zip(df_sst_a_melt['Lat'], df_sst_a_melt['Long']))\n",
        "\n",
        "# Find the common coordinates\n",
        "common_coords = pacific_coords.intersection(npp_coords)\n",
        "\n",
        "print(f\"Common coordinates between Pacific and df_npp: {len(common_coords)}\")"
      ],
      "metadata": {
        "id": "aC-JBeobuCwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge"
      ],
      "metadata": {
        "id": "HawZhcHOuHck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "merged_dfs = []  # List to store merged dataframes\n",
        "\n",
        "for ocean_name, ocean_df in zip(ocean_names, ocean_dfs):\n",
        "    # merge ocean_df with all dataframes in df_melted_list\n",
        "    for df_melted in df_melted_list:\n",
        "        ocean_df = pd.merge(ocean_df, df_melted, on=['Lat', 'Long', 'Year'], how='left')\n",
        "\n",
        "    # Fill NaN values with the mean of each column\n",
        "    for col in ocean_df.select_dtypes(include=[np.number]).columns:\n",
        "        ocean_df[col] = ocean_df[col].astype(float)\n",
        "        ocean_df[col].fillna(ocean_df[col].mean(), inplace=True)\n",
        "        if ocean_df[col].apply(float.is_integer).all():  # Check if all values are integer\n",
        "            ocean_df[col] = ocean_df[col].astype('Int64')  # Change dtype back to integer\n",
        "\n",
        "    merged_dfs.append(ocean_df)  # Append the merged dataframe to the list\n",
        "\n",
        "with pd.ExcelWriter('merged_sst_a.xlsx') as writer:\n",
        "    for ocean_name, merged_df in zip(ocean_names, merged_dfs):\n",
        "        merged_df.to_excel(writer, sheet_name=ocean_name)  # Write each merged dataframe to a different sheet"
      ],
      "metadata": {
        "id": "E8Bkcwl_uIYs"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}