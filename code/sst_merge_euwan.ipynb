{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### MODIS (1080x2160)"
      ],
      "metadata": {
        "id": "IAp7O9WS_gay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data from MODIS (1080x2160):  http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.sst.modis.php\n",
        "\n",
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "# Make sure 'data/' directory exists\n",
        "if not os.path.exists('data/'):\n",
        "    os.makedirs('data/')\n",
        "\n",
        "modis_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/sst.modis.r2022/hdf/sst.m.{}.tar'\n",
        "\n",
        "for year in range(2002, 2023):  # 2023 is not included\n",
        "    url = modis_url.format(year)\n",
        "    filename = f'data/sst.m.{year}.tar'\n",
        "    download_file(url, filename)\n",
        "    print(f\"Downloaded: {filename}\")\n",
        "\n",
        "\n",
        "# Download the satellite data (1080x2160): http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.m.chl.m.sst.php\n",
        "\n",
        "base_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/vgpm.r2022.m.chl.m.sst/hdf/vgpm.m.{}.tar'\n",
        "\n",
        "for year in range(2002, 2023):  # 2023 is not included\n",
        "    url = base_url.format(year)\n",
        "    filename = f'data/vgpm.m.{year}.tar'\n",
        "    download_file(url, filename)\n",
        "    print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RvXMDbHKtgb",
        "outputId": "f5213f3a-32f3-4eca-e2de-7b4659177004"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: data/sst.m.2002.tar\n",
            "Downloaded: data/sst.m.2003.tar\n",
            "Downloaded: data/sst.m.2004.tar\n",
            "Downloaded: data/sst.m.2005.tar\n",
            "Downloaded: data/sst.m.2006.tar\n",
            "Downloaded: data/sst.m.2007.tar\n",
            "Downloaded: data/sst.m.2008.tar\n",
            "Downloaded: data/sst.m.2009.tar\n",
            "Downloaded: data/sst.m.2010.tar\n",
            "Downloaded: data/sst.m.2011.tar\n",
            "Downloaded: data/sst.m.2012.tar\n",
            "Downloaded: data/sst.m.2013.tar\n",
            "Downloaded: data/sst.m.2014.tar\n",
            "Downloaded: data/sst.m.2015.tar\n",
            "Downloaded: data/sst.m.2016.tar\n",
            "Downloaded: data/sst.m.2017.tar\n",
            "Downloaded: data/sst.m.2018.tar\n",
            "Downloaded: data/sst.m.2019.tar\n",
            "Downloaded: data/sst.m.2020.tar\n",
            "Downloaded: data/sst.m.2021.tar\n",
            "Downloaded: data/sst.m.2022.tar\n",
            "Downloaded: data/vgpm.m.2002.tar\n",
            "Downloaded: data/vgpm.m.2003.tar\n",
            "Downloaded: data/vgpm.m.2004.tar\n",
            "Downloaded: data/vgpm.m.2005.tar\n",
            "Downloaded: data/vgpm.m.2006.tar\n",
            "Downloaded: data/vgpm.m.2007.tar\n",
            "Downloaded: data/vgpm.m.2008.tar\n",
            "Downloaded: data/vgpm.m.2009.tar\n",
            "Downloaded: data/vgpm.m.2010.tar\n",
            "Downloaded: data/vgpm.m.2011.tar\n",
            "Downloaded: data/vgpm.m.2012.tar\n",
            "Downloaded: data/vgpm.m.2013.tar\n",
            "Downloaded: data/vgpm.m.2014.tar\n",
            "Downloaded: data/vgpm.m.2015.tar\n",
            "Downloaded: data/vgpm.m.2016.tar\n",
            "Downloaded: data/vgpm.m.2017.tar\n",
            "Downloaded: data/vgpm.m.2018.tar\n",
            "Downloaded: data/vgpm.m.2019.tar\n",
            "Downloaded: data/vgpm.m.2020.tar\n",
            "Downloaded: data/vgpm.m.2021.tar\n",
            "Downloaded: data/vgpm.m.2022.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the satellite data and MODIS data\n",
        "# Issue: later while calculating mean of merged data, sst no longer exists\n",
        "import subprocess\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Directory to store the merged TAR files\n",
        "merged_dir = \"merged_tars\"\n",
        "os.makedirs(merged_dir, exist_ok=True)\n",
        "\n",
        "# Iterate over the years and merge TAR files by year\n",
        "for year in range(2002, 2023):\n",
        "    sst_tar_file = f\"data/sst.m.{year}.tar\"\n",
        "    vgpm_tar_file = f\"data/vgpm.m.{year}.tar\"\n",
        "    \n",
        "    # Name of merged TAR file\n",
        "    filename = os.path.join(merged_dir, f\"{year}.tar\")\n",
        "    \n",
        "    # Command to merge TAR files\n",
        "    command = [\"tar\", \"-cf\", filename, sst_tar_file, vgpm_tar_file]\n",
        "    subprocess.run(command)    \n",
        "    print(f\"Merge complete for {year}.\")\n",
        "\n",
        "    # Download the merged TAR file\n",
        "    download_file(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpNYQvxgSEHq",
        "outputId": "eb4dc4ea-9225-42d5-b480-e0e95d9200a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge complete for year 2002.\n",
            "Merge complete for year 2003.\n",
            "Merge complete for year 2004.\n",
            "Merge complete for year 2005.\n",
            "Merge complete for year 2006.\n",
            "Merge complete for year 2007.\n",
            "Merge complete for year 2008.\n",
            "Merge complete for year 2009.\n",
            "Merge complete for year 2010.\n",
            "Merge complete for year 2011.\n",
            "Merge complete for year 2012.\n",
            "Merge complete for year 2013.\n",
            "Merge complete for year 2014.\n",
            "Merge complete for year 2015.\n",
            "Merge complete for year 2016.\n",
            "Merge complete for year 2017.\n",
            "Merge complete for year 2018.\n",
            "Merge complete for year 2019.\n",
            "Merge complete for year 2020.\n",
            "Merge complete for year 2021.\n",
            "Merge complete for year 2022.\n",
            "All merges complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SeaWIFS/AVHRR (1024x2048) <-- ISSUE: SeaWIFS VGPM and AVHRR are not the same resolution"
      ],
      "metadata": {
        "id": "LbOSHsVJX3oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data from SeaWIFS/AVHRR (1024x2048): http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.sst.avhrr.php\n",
        "\n",
        "seawifs_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/sst.avhrr/hdf/sst.a.{}.tar'\n",
        "\n",
        "for year in range(1997, 2002):  # 2003 is not included\n",
        "    url = seawifs_url.format(year)\n",
        "    filename = f'data/sst.a.{year}.tar'\n",
        "    download_file(url, filename)\n",
        "    print(f\"Downloaded: {filename}\")\n",
        "\n",
        "# Download satellite data (1080x2160): http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.s.chl.a.sst.php\n",
        "\n",
        "new_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/vgpm.r2014.s.chl.a.sst/hdf/vgpm.s.{}.tar'\n",
        "\n",
        "for year in range(1997, 2002):  # 2023 is not included\n",
        "    url = new_url.format(year)\n",
        "    filename = f'data/vgpm.s.{year}.tar'\n",
        "    download_file(url, filename)\n",
        "    print(f\"Downloaded: {filename}\")\n",
        "\n",
        "# Merge the satellite data and SeaWIFS/AVHRR data\n",
        "# Waiting to resolve resolution issue"
      ],
      "metadata": {
        "id": "wzE1SCPIIE_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14efe5db-9fba-4236-c091-8f0493198ee3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: data/sst.a.1997.tar\n",
            "Downloaded: data/sst.a.1998.tar\n",
            "Downloaded: data/sst.a.1999.tar\n",
            "Downloaded: data/sst.a.2000.tar\n",
            "Downloaded: data/sst.a.2001.tar\n",
            "Downloaded: data/sst.a.2002.tar\n",
            "Downloaded: data/vgpm.s.1997.tar\n",
            "Downloaded: data/vgpm.s.1998.tar\n",
            "Downloaded: data/vgpm.s.1999.tar\n",
            "Downloaded: data/vgpm.s.2000.tar\n",
            "Downloaded: data/vgpm.s.2001.tar\n",
            "Downloaded: data/vgpm.s.2002.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating MODIS Mean"
      ],
      "metadata": {
        "id": "TmSnw-xY_9z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import tempfile\n",
        "!pip install pyhdf\n",
        "from pyhdf.SD import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Loop through each tar file\n",
        "for year in range(2002, 2023):\n",
        "    # Open the tar file\n",
        "    tar_filename = f'data/sst.m.{year}.tar'\n",
        "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
        "        # Loop through each member of the tar file\n",
        "        for member in tar.getmembers():\n",
        "            # If it's a .hdf.gz file\n",
        "            if member.name.endswith('.hdf.gz'):\n",
        "                # Open the .hdf.gz file\n",
        "                f = tar.extractfile(member)\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    # Decompress the .hdf.gz file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile() as tmp:\n",
        "                        tmp.write(gz.read())\n",
        "                        tmp.seek(0)  # Go back to the start of the file\n",
        "\n",
        "                        # Open the temporary .hdf file\n",
        "                        hdf_file = SD(tmp.name, SDC.READ)\n",
        "\n",
        "                        # Access the 'sst' dataset\n",
        "                        data = hdf_file.select('sst')[:]\n",
        "\n",
        "                        # Take the mean along the time (2nd) dimension and reshape to a 1D array\n",
        "                        mean_data = np.mean(data, axis=1).reshape(-1)\n",
        "\n",
        "                        # Add this to the DataFrame\n",
        "                        df[str(year)] = mean_data\n",
        "\n",
        "                        # Close the file\n",
        "                        hdf_file.end()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('sst_m_means.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WrKSu79_-O1",
        "outputId": "5a084556-f8f0-4d98-c01e-f8331aaa1020"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyhdf\n",
            "  Downloading pyhdf-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.8/739.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyhdf) (1.22.4)\n",
            "Installing collected packages: pyhdf\n",
            "Successfully installed pyhdf-0.10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the results\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Loop through each tar file\n",
        "for year in range(2002, 2023):\n",
        "    # Open the tar file\n",
        "    tar_filename = f'data/sst.m.{year}.tar'\n",
        "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
        "        # Loop through each member of the tar file\n",
        "        for member in tar.getmembers():\n",
        "            # If it's a .hdf.gz file\n",
        "            if member.name.endswith('.hdf.gz'):\n",
        "                # Open the .hdf.gz file\n",
        "                f = tar.extractfile(member)\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    # Decompress the .hdf.gz file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile() as tmp:\n",
        "                        tmp.write(gz.read())\n",
        "                        tmp.seek(0)  # Go back to the start of the file\n",
        "\n",
        "                        # Open the temporary .hdf file\n",
        "                        hdf_file = SD(tmp.name, SDC.READ)\n",
        "\n",
        "                        # Access the 'sst' dataset\n",
        "                        data = hdf_file.select('sst')\n",
        "\n",
        "                        # Retrieve the latitude and longitude attributes\n",
        "                        latitude_center = data.attributes()['Latitude Center']\n",
        "                        longitude_center = data.attributes()['Longitude Center']\n",
        "\n",
        "                        # Take the mean along the time (2nd) dimension\n",
        "                        mean_data = np.mean(data[:], axis=1).reshape(-1)\n",
        "\n",
        "                        # Add this to the DataFrame\n",
        "                        df_temp = pd.DataFrame({\n",
        "                            'year': [year] * len(mean_data),\n",
        "                            'latitude': [latitude_center] * len(mean_data),\n",
        "                            'longitude': [longitude_center] * len(mean_data),\n",
        "                            'sst': mean_data\n",
        "                        })\n",
        "                        df = pd.concat([df, df_temp])\n",
        "\n",
        "                        # Close the file\n",
        "                        hdf_file.end()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('sst_m_means_new.csv', index=False)\n"
      ],
      "metadata": {
        "id": "bdzbkFHHAMf7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Merged MODIS and Satellite Mean"
      ],
      "metadata": {
        "id": "BL_U9UUBYftg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Waiting to resolve merge issue\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import tempfile\n",
        "!pip install pyhdf\n",
        "from pyhdf.SD import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Loop through each tar file\n",
        "for year in range(2002, 2023):\n",
        "    # Open the tar file\n",
        "    tar_filename = f'merged_tars/{year}.tar'\n",
        "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
        "        # Loop through each member of the tar file\n",
        "        for member in tar.getmembers():\n",
        "            # If it's a .hdf.gz file\n",
        "            if member.name.endswith('.hdf.gz'):\n",
        "                # Open the .hdf.gz file\n",
        "                f = tar.extractfile(member)\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    # Decompress the .hdf.gz file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile() as tmp:\n",
        "                        tmp.write(gz.read())\n",
        "                        tmp.seek(0)  # Go back to the start of the file\n",
        "\n",
        "                        # Open the temporary .hdf file\n",
        "                        hdf_file = SD(tmp.name, SDC.READ)\n",
        "\n",
        "                        # Access the 'sst' dataset\n",
        "                        data = hdf_file.select('sst')[:]\n",
        "\n",
        "                        # Take the mean along the time (2nd) dimension and reshape to a 1D array\n",
        "                        mean_data = np.mean(data, axis=1).reshape(-1)\n",
        "\n",
        "                        # Add this to the DataFrame\n",
        "                        df[str(year)] = mean_data\n",
        "\n",
        "                        # Close the file\n",
        "                        hdf_file.end()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('merged_sst_m_means.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "JVoHfx81Yh2M",
        "outputId": "e7d7bc5d-e721-430b-a735-79a0e7277d55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyhdf in /usr/local/lib/python3.10/dist-packages (0.10.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyhdf) (1.22.4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HDF4Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHDF4Error\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyhdf/SD.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, name_or_index)\u001b[0m\n\u001b[1;32m   1620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnametoindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHDF4Error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyhdf/SD.py\u001b[0m in \u001b[0;36mnametoindex\u001b[0;34m(self, sds_name)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0msds_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSDnametoindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msds_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0m_checkErr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nametoindex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msds_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'non existent SDS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msds_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyhdf/error.py\u001b[0m in \u001b[0;36m_checkErr\u001b[0;34m(procName, val, msg)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s : %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprocName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHDF4Error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mHDF4Error\u001b[0m: nametoindex : non existent SDS",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHDF4Error\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fa295b8516f4>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0;31m# Access the 'sst' dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sst4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0;31m# Take the mean along the time (2nd) dimension and reshape to a 1D array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyhdf/SD.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, name_or_index)\u001b[0m\n\u001b[1;32m   1621\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnametoindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHDF4Error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mHDF4Error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select: non-existent dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSDselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m         \u001b[0m_checkErr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'select'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cannot execute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHDF4Error\u001b[0m: select: non-existent dataset"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating SeaWIFS/AVHRR Mean"
      ],
      "metadata": {
        "id": "0L6TI9gnYKrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Waiting to resolve merge issue\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import tempfile\n",
        "!pip install pyhdf\n",
        "from pyhdf.SD import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Loop through each tar file\n",
        "for year in range(1997, 2002):\n",
        "    # Open the tar file\n",
        "    tar_filename = f'data/sst.a.{year}.tar'\n",
        "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
        "        # Loop through each member of the tar file\n",
        "        for member in tar.getmembers():\n",
        "            # If it's a .hdf.gz file\n",
        "            if member.name.endswith('.hdf.gz'):\n",
        "                # Open the .hdf.gz file\n",
        "                f = tar.extractfile(member)\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    # Decompress the .hdf.gz file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile() as tmp:\n",
        "                        tmp.write(gz.read())\n",
        "                        tmp.seek(0)  # Go back to the start of the file\n",
        "\n",
        "                        # Open the temporary .hdf file\n",
        "                        hdf_file = SD(tmp.name, SDC.READ)\n",
        "\n",
        "                        # Access the 'sst' dataset\n",
        "                        data = hdf_file.select('sst')[:]\n",
        "\n",
        "                        # Take the mean along the time (2nd) dimension and reshape to a 1D array\n",
        "                        mean_data = np.mean(data, axis=1).reshape(-1)\n",
        "\n",
        "                        # Add this to the DataFrame\n",
        "                        df[str(year)] = mean_data\n",
        "\n",
        "                        # Close the file\n",
        "                        hdf_file.end()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('sst_a_means.csv', index=False)"
      ],
      "metadata": {
        "id": "15vZ1aTrY_q4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e818ada6-6843-4cac-edd4-c6140cd35ba2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyhdf in /usr/local/lib/python3.10/dist-packages (0.10.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyhdf) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the results\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Loop through each tar file\n",
        "for year in range(1997, 2002):\n",
        "    # Open the tar file\n",
        "    tar_filename = f'data/sst.a.{year}.tar'\n",
        "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
        "        # Loop through each member of the tar file\n",
        "        for member in tar.getmembers():\n",
        "            # If it's a .hdf.gz file\n",
        "            if member.name.endswith('.hdf.gz'):\n",
        "                # Open the .hdf.gz file\n",
        "                f = tar.extractfile(member)\n",
        "                with gzip.open(f, 'rb') as gz:\n",
        "                    # Decompress the .hdf.gz file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile() as tmp:\n",
        "                        tmp.write(gz.read())\n",
        "                        tmp.seek(0)  # Go back to the start of the file\n",
        "\n",
        "                        # Open the temporary .hdf file\n",
        "                        hdf_file = SD(tmp.name, SDC.READ)\n",
        "\n",
        "                        # Access the 'sst' dataset\n",
        "                        data = hdf_file.select('sst')\n",
        "\n",
        "                        # Retrieve the latitude and longitude attributes\n",
        "                        latitude_center = data.attributes()['Latitude Center']\n",
        "                        longitude_center = data.attributes()['Longitude Center']\n",
        "\n",
        "                        # Take the mean along the time (2nd) dimension\n",
        "                        mean_data = np.mean(data[:], axis=1).reshape(-1)\n",
        "\n",
        "                        # Add this to the DataFrame\n",
        "                        df_temp = pd.DataFrame({\n",
        "                            'year': [year] * len(mean_data),\n",
        "                            'latitude': [latitude_center] * len(mean_data),\n",
        "                            'longitude': [longitude_center] * len(mean_data),\n",
        "                            'sst': mean_data\n",
        "                        })\n",
        "                        df = pd.concat([df, df_temp])\n",
        "\n",
        "                        # Close the file\n",
        "                        hdf_file.end()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('sst_a_means_new.csv', index=False)"
      ],
      "metadata": {
        "id": "t3k09XCzeJZl"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}