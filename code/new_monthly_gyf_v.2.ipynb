{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# read new data\n",
    "# read each page from excel file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filename='cleaned'\n",
    "Pacific=pd.read_excel(filename+'.xlsx',sheet_name='Pacific')\n",
    "Atlantic=pd.read_excel(filename+'.xlsx',sheet_name='Atlantic')\n",
    "Mediterranean=pd.read_excel(filename+'.xlsx',sheet_name='Mediterranean')\n",
    "Southern=pd.read_excel(filename+'.xlsx',sheet_name='Southern Ocean')\n",
    "Arctic=pd.read_excel(filename+'.xlsx',sheet_name='Arctic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1343268829.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Lat'] = df2['Lat'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1343268829.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Long'] = df2['Long'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1343268829.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Lat'] = df2['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1343268829.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Long'] = df2['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# we will be only merge one ocean at a time \n",
    "\n",
    "df2 = Pacific[['Lat','Long','Year']]\n",
    "# convert to 'Lat','Long' string \n",
    "df2['Lat'] = df2['Lat'].astype(str)\n",
    "df2['Long'] = df2['Long'].astype(str)\n",
    "\n",
    "# here shockingly we have chinese character in the data!\n",
    "df2['Lat'] = df2['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "df2['Long'] = df2['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "# drop nan in lat and long \n",
    "df2 = df2.dropna(subset=['Lat','Long'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "import gzip\n",
    "from pyhdf.SD import SD, SDC\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# List of all the years\n",
    "years = range(1997, 2024)\n",
    "\n",
    "# Empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Get the original latitudes and longitudes\n",
    "orig_lats = df2['Lat'].copy()\n",
    "orig_lons = df2['Long'].copy()\n",
    "\n",
    "for year in years:\n",
    "    with tarfile.open(f'data/par.s.{year}.tar', \"r:\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Decide month from the filename=day of year\n",
    "                # last three integers\n",
    "                day_of_year = int(str(member.name.split('.')[1])[-3:])\n",
    "                month = str((day_of_year - 1) // 30 + 1).zfill(2)\n",
    "\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "                        # Access the 'npp' dataset\n",
    "                        data = hdf_file.select('par')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Define the latitude and longitude arrays\n",
    "                        lats = np.linspace(90, -90, data.shape[0])  # Shape[0] is the number of rows\n",
    "                        lons = np.linspace(-180, 180, data.shape[1])  # Shape[1] is the number of columns\n",
    "\n",
    "                        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "                        lat_list = lat_grid.reshape(-1,1)\n",
    "                        lon_list = lon_grid.reshape(-1,1)\n",
    "                        points = np.concatenate([lat_list, lon_list], axis=1)\n",
    "                        xi = np.array(list(zip(df2['Lat'], df2['Long'])))\n",
    "                        df2[f'par_{month}'] = griddata(points, data.ravel(), xi, method='nearest')\n",
    "\n",
    "        # Add the year column\n",
    "        df2['Year'] = year\n",
    "        df2['Lat'] = orig_lats\n",
    "        df2['Long'] = orig_lons\n",
    "\n",
    "        # Append df2 to the results DataFrame\n",
    "        all_results = pd.concat([all_results, df2])\n",
    "\n",
    "    # Close the file\n",
    "    hdf_file.end()\n",
    "\n",
    "# save the data\n",
    "all_results.to_csv('par_means_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of yearly mean\n",
    "all_results['yearly_mean_par'] = all_results.filter(regex='par_').mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of Cr_nmol/kg from the 'cleaned' excel file\n",
    "all_results['Cr_nmol/kg'] = Pacific['Cr_nmol/kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 'Ocean' column filled with \"Pacific\" to the df_ocean_processed DataFrame\n",
    "all_results['Ocean'] = 'Pacific'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data csv\n",
    "all_results.to_csv('par_means_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Year</th>\n",
       "      <th>par_10</th>\n",
       "      <th>par_11</th>\n",
       "      <th>par_12</th>\n",
       "      <th>par_01</th>\n",
       "      <th>par_02</th>\n",
       "      <th>par_04</th>\n",
       "      <th>par_05</th>\n",
       "      <th>par_06</th>\n",
       "      <th>par_07</th>\n",
       "      <th>par_08</th>\n",
       "      <th>par_09</th>\n",
       "      <th>par_03</th>\n",
       "      <th>yearly_mean_par</th>\n",
       "      <th>Cr_nmol/kg</th>\n",
       "      <th>Ocean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>21.476667</td>\n",
       "      <td>-126.595000</td>\n",
       "      <td>2015</td>\n",
       "      <td>43.563354</td>\n",
       "      <td>30.265165</td>\n",
       "      <td>25.576950</td>\n",
       "      <td>28.543900</td>\n",
       "      <td>48.103226</td>\n",
       "      <td>51.524967</td>\n",
       "      <td>52.658382</td>\n",
       "      <td>50.613980</td>\n",
       "      <td>50.346275</td>\n",
       "      <td>51.223625</td>\n",
       "      <td>48.067580</td>\n",
       "      <td>43.436516</td>\n",
       "      <td>43.660328</td>\n",
       "      <td>3.065813</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14012</th>\n",
       "      <td>24.283333</td>\n",
       "      <td>-114.983333</td>\n",
       "      <td>2023</td>\n",
       "      <td>42.392548</td>\n",
       "      <td>35.729366</td>\n",
       "      <td>30.058996</td>\n",
       "      <td>34.886550</td>\n",
       "      <td>50.883163</td>\n",
       "      <td>53.515568</td>\n",
       "      <td>48.803883</td>\n",
       "      <td>52.457397</td>\n",
       "      <td>57.861660</td>\n",
       "      <td>52.007725</td>\n",
       "      <td>47.847650</td>\n",
       "      <td>41.399918</td>\n",
       "      <td>45.653700</td>\n",
       "      <td>2.595384</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>-158.000000</td>\n",
       "      <td>2009</td>\n",
       "      <td>13.996820</td>\n",
       "      <td>10.336063</td>\n",
       "      <td>9.007836</td>\n",
       "      <td>8.566417</td>\n",
       "      <td>24.176481</td>\n",
       "      <td>29.687132</td>\n",
       "      <td>37.433030</td>\n",
       "      <td>43.771500</td>\n",
       "      <td>35.645190</td>\n",
       "      <td>31.108965</td>\n",
       "      <td>24.761683</td>\n",
       "      <td>24.246418</td>\n",
       "      <td>24.394796</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>24.283333</td>\n",
       "      <td>-114.983333</td>\n",
       "      <td>1998</td>\n",
       "      <td>40.360960</td>\n",
       "      <td>33.517483</td>\n",
       "      <td>28.484533</td>\n",
       "      <td>30.729593</td>\n",
       "      <td>44.247433</td>\n",
       "      <td>53.067700</td>\n",
       "      <td>52.799720</td>\n",
       "      <td>56.900566</td>\n",
       "      <td>52.312920</td>\n",
       "      <td>52.691704</td>\n",
       "      <td>46.944910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.732506</td>\n",
       "      <td>2.741385</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6416</th>\n",
       "      <td>-14.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>2008</td>\n",
       "      <td>52.646350</td>\n",
       "      <td>49.477196</td>\n",
       "      <td>57.983517</td>\n",
       "      <td>55.068720</td>\n",
       "      <td>54.780950</td>\n",
       "      <td>47.975530</td>\n",
       "      <td>41.874190</td>\n",
       "      <td>35.072770</td>\n",
       "      <td>36.830692</td>\n",
       "      <td>40.945404</td>\n",
       "      <td>48.442265</td>\n",
       "      <td>52.546062</td>\n",
       "      <td>47.803640</td>\n",
       "      <td>2.997392</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>46.071667</td>\n",
       "      <td>-129.928333</td>\n",
       "      <td>2002</td>\n",
       "      <td>20.309692</td>\n",
       "      <td>11.539831</td>\n",
       "      <td>6.599642</td>\n",
       "      <td>8.220115</td>\n",
       "      <td>23.994345</td>\n",
       "      <td>35.790867</td>\n",
       "      <td>45.739395</td>\n",
       "      <td>44.754710</td>\n",
       "      <td>45.832428</td>\n",
       "      <td>41.778934</td>\n",
       "      <td>32.093483</td>\n",
       "      <td>24.329458</td>\n",
       "      <td>28.415243</td>\n",
       "      <td>4.439024</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>-34.670000</td>\n",
       "      <td>154.550000</td>\n",
       "      <td>1999</td>\n",
       "      <td>42.891700</td>\n",
       "      <td>51.410990</td>\n",
       "      <td>51.152927</td>\n",
       "      <td>51.264618</td>\n",
       "      <td>37.401585</td>\n",
       "      <td>24.850351</td>\n",
       "      <td>22.412460</td>\n",
       "      <td>14.926084</td>\n",
       "      <td>18.127250</td>\n",
       "      <td>26.553587</td>\n",
       "      <td>35.294075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.207787</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>-132.000000</td>\n",
       "      <td>2011</td>\n",
       "      <td>41.710384</td>\n",
       "      <td>29.976696</td>\n",
       "      <td>24.854902</td>\n",
       "      <td>30.439160</td>\n",
       "      <td>43.377160</td>\n",
       "      <td>44.727130</td>\n",
       "      <td>44.051098</td>\n",
       "      <td>48.882400</td>\n",
       "      <td>50.035355</td>\n",
       "      <td>42.145756</td>\n",
       "      <td>44.395580</td>\n",
       "      <td>40.688656</td>\n",
       "      <td>40.440353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10640</th>\n",
       "      <td>-34.670000</td>\n",
       "      <td>154.550000</td>\n",
       "      <td>2016</td>\n",
       "      <td>46.290485</td>\n",
       "      <td>59.511734</td>\n",
       "      <td>59.187420</td>\n",
       "      <td>51.391903</td>\n",
       "      <td>51.658516</td>\n",
       "      <td>31.805063</td>\n",
       "      <td>23.117966</td>\n",
       "      <td>15.132629</td>\n",
       "      <td>19.427675</td>\n",
       "      <td>26.529966</td>\n",
       "      <td>31.337168</td>\n",
       "      <td>42.379513</td>\n",
       "      <td>38.147503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>-30.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>2021</td>\n",
       "      <td>43.398933</td>\n",
       "      <td>51.341602</td>\n",
       "      <td>55.577160</td>\n",
       "      <td>58.992180</td>\n",
       "      <td>44.996517</td>\n",
       "      <td>32.357500</td>\n",
       "      <td>23.429804</td>\n",
       "      <td>21.201480</td>\n",
       "      <td>22.145530</td>\n",
       "      <td>30.605125</td>\n",
       "      <td>39.291798</td>\n",
       "      <td>44.816162</td>\n",
       "      <td>39.012817</td>\n",
       "      <td>3.183553</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Lat        Long  Year     par_10     par_11     par_12  \\\n",
       "9671   21.476667 -126.595000  2015  43.563354  30.265165  25.576950   \n",
       "14012  24.283333 -114.983333  2023  42.392548  35.729366  30.058996   \n",
       "6682   46.000000 -158.000000  2009  13.996820  10.336063   9.007836   \n",
       "614    24.283333 -114.983333  1998  40.360960  33.517483  28.484533   \n",
       "6416  -14.000000  -99.000000  2008  52.646350  49.477196  57.983517   \n",
       "2869   46.071667 -129.928333  2002  20.309692  11.539831   6.599642   \n",
       "1476  -34.670000  154.550000  1999  42.891700  51.410990  51.152927   \n",
       "7504   21.000000 -132.000000  2011  41.710384  29.976696  24.854902   \n",
       "10640 -34.670000  154.550000  2016  46.290485  59.511734  59.187420   \n",
       "13300 -30.000000  175.000000  2021  43.398933  51.341602  55.577160   \n",
       "\n",
       "          par_01     par_02     par_04     par_05     par_06     par_07  \\\n",
       "9671   28.543900  48.103226  51.524967  52.658382  50.613980  50.346275   \n",
       "14012  34.886550  50.883163  53.515568  48.803883  52.457397  57.861660   \n",
       "6682    8.566417  24.176481  29.687132  37.433030  43.771500  35.645190   \n",
       "614    30.729593  44.247433  53.067700  52.799720  56.900566  52.312920   \n",
       "6416   55.068720  54.780950  47.975530  41.874190  35.072770  36.830692   \n",
       "2869    8.220115  23.994345  35.790867  45.739395  44.754710  45.832428   \n",
       "1476   51.264618  37.401585  24.850351  22.412460  14.926084  18.127250   \n",
       "7504   30.439160  43.377160  44.727130  44.051098  48.882400  50.035355   \n",
       "10640  51.391903  51.658516  31.805063  23.117966  15.132629  19.427675   \n",
       "13300  58.992180  44.996517  32.357500  23.429804  21.201480  22.145530   \n",
       "\n",
       "          par_08     par_09     par_03  yearly_mean_par  Cr_nmol/kg    Ocean  \n",
       "9671   51.223625  48.067580  43.436516        43.660328    3.065813  Pacific  \n",
       "14012  52.007725  47.847650  41.399918        45.653700    2.595384  Pacific  \n",
       "6682   31.108965  24.761683  24.246418        24.394796    3.080000  Pacific  \n",
       "614    52.691704  46.944910        NaN        44.732506    2.741385  Pacific  \n",
       "6416   40.945404  48.442265  52.546062        47.803640    2.997392  Pacific  \n",
       "2869   41.778934  32.093483  24.329458        28.415243    4.439024  Pacific  \n",
       "1476   26.553587  35.294075        NaN        34.207787    3.750000  Pacific  \n",
       "7504   42.145756  44.395580  40.688656        40.440353         NaN  Pacific  \n",
       "10640  26.529966  31.337168  42.379513        38.147503         NaN  Pacific  \n",
       "13300  30.605125  39.291798  44.816162        39.012817    3.183553  Pacific  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the data\n",
    "df_par_means = pd.read_csv('par_means_1.csv')\n",
    "df_par_means.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/3129857474.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Lat'] = df3['Lat'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/3129857474.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Long'] = df3['Long'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/3129857474.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Lat'] = df3['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/3129857474.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Long'] = df3['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# we will be only merge one ocean at a time \n",
    "\n",
    "df3 = Atlantic[['Lat','Long','Year']]\n",
    "# convert to 'Lat','Long' string \n",
    "df3['Lat'] = df3['Lat'].astype(str)\n",
    "df3['Long'] = df3['Long'].astype(str)\n",
    "\n",
    "# here shockingly we have chinese character in the data!\n",
    "df3['Lat'] = df3['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "df3['Long'] = df3['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "# drop nan in lat and long \n",
    "df3 = df3.dropna(subset=['Lat','Long'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "import gzip\n",
    "from pyhdf.SD import SD, SDC\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# List of all the years\n",
    "years = range(1997, 2024)\n",
    "\n",
    "# Empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Get the original latitudes and longitudes\n",
    "orig_lats = df3['Lat'].copy()\n",
    "orig_lons = df3['Long'].copy()\n",
    "\n",
    "for year in years:\n",
    "    with tarfile.open(f'data/par.s.{year}.tar', \"r:\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Decide month from the filename=day of year\n",
    "                # last three integers\n",
    "                day_of_year = int(str(member.name.split('.')[1])[-3:])\n",
    "                month = str((day_of_year - 1) // 30 + 1).zfill(2)\n",
    "\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "                        # Access the 'npp' dataset\n",
    "                        data = hdf_file.select('par')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Define the latitude and longitude arrays\n",
    "                        lats = np.linspace(90, -90, data.shape[0])  # Shape[0] is the number of rows\n",
    "                        lons = np.linspace(-180, 180, data.shape[1])  # Shape[1] is the number of columns\n",
    "\n",
    "                        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "                        lat_list = lat_grid.reshape(-1,1)\n",
    "                        lon_list = lon_grid.reshape(-1,1)\n",
    "                        points = np.concatenate([lat_list, lon_list], axis=1)\n",
    "                        xi = np.array(list(zip(df3['Lat'], df3['Long'])))\n",
    "                        df3[f'par_{month}'] = griddata(points, data.ravel(), xi, method='nearest')\n",
    "\n",
    "        # Add the year column\n",
    "        df3['Year'] = year\n",
    "        df3['Lat'] = orig_lats\n",
    "        df3['Long'] = orig_lons\n",
    "\n",
    "        # Append df2 to the results DataFrame\n",
    "        all_results = pd.concat([all_results, df3])\n",
    "\n",
    "    # Close the file\n",
    "    hdf_file.end()\n",
    "\n",
    "# save the data\n",
    "all_results.to_csv('par_means_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of yearly mean\n",
    "all_results['yearly_mean_par'] = all_results.filter(regex='par_').mean(axis=1)\n",
    "# add a column of Cr_nmol/kg from the 'cleaned' excel file\n",
    "all_results['Cr_nmol/kg'] = Atlantic['Cr_nmol/kg']\n",
    "# create the 'Ocean' column filled with \"Pacific\" to the df_ocean_processed DataFrame\n",
    "all_results['Ocean'] = 'Atlantic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data csv\n",
    "all_results.to_csv('par_means_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1145701115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['Lat'] = df4['Lat'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1145701115.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['Long'] = df4['Long'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1145701115.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['Lat'] = df4['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1145701115.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['Long'] = df4['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# we will be only merge one ocean at a time \n",
    "\n",
    "df4 = Mediterranean[['Lat','Long','Year']]\n",
    "# convert to 'Lat','Long' string \n",
    "df4['Lat'] = df4['Lat'].astype(str)\n",
    "df4['Long'] = df4['Long'].astype(str)\n",
    "\n",
    "# here shockingly we have chinese character in the data!\n",
    "df4['Lat'] = df4['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "df4['Long'] = df4['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "# drop nan in lat and long \n",
    "df4 = df4.dropna(subset=['Lat','Long'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the years\n",
    "years = range(1997, 2024)\n",
    "\n",
    "# Empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Get the original latitudes and longitudes\n",
    "orig_lats = df4['Lat'].copy()\n",
    "orig_lons = df4['Long'].copy()\n",
    "\n",
    "for year in years:\n",
    "    with tarfile.open(f'data/par.s.{year}.tar', \"r:\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Decide month from the filename=day of year\n",
    "                # last three integers\n",
    "                day_of_year = int(str(member.name.split('.')[1])[-3:])\n",
    "                month = str((day_of_year - 1) // 30 + 1).zfill(2)\n",
    "\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "                        # Access the 'npp' dataset\n",
    "                        data = hdf_file.select('par')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Define the latitude and longitude arrays\n",
    "                        lats = np.linspace(90, -90, data.shape[0])  # Shape[0] is the number of rows\n",
    "                        lons = np.linspace(-180, 180, data.shape[1])  # Shape[1] is the number of columns\n",
    "\n",
    "                        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "                        lat_list = lat_grid.reshape(-1,1)\n",
    "                        lon_list = lon_grid.reshape(-1,1)\n",
    "                        points = np.concatenate([lat_list, lon_list], axis=1)\n",
    "                        xi = np.array(list(zip(df4['Lat'], df4['Long'])))\n",
    "                        df4[f'par_{month}'] = griddata(points, data.ravel(), xi, method='nearest')\n",
    "\n",
    "        # Add the year column\n",
    "        df4['Year'] = year\n",
    "        df4['Lat'] = orig_lats\n",
    "        df4['Long'] = orig_lons\n",
    "\n",
    "        # Append df2 to the results DataFrame\n",
    "        all_results = pd.concat([all_results, df4])\n",
    "\n",
    "    # Close the file\n",
    "    hdf_file.end()\n",
    "\n",
    "# save the data\n",
    "all_results.to_csv('par_means_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of yearly mean\n",
    "all_results['yearly_mean_par'] = all_results.filter(regex='par_').mean(axis=1)\n",
    "# add a column of Cr_nmol/kg from the 'cleaned' excel file\n",
    "all_results['Cr_nmol/kg'] = Mediterranean['Cr_nmol/kg']\n",
    "# create the 'Ocean' column filled with \"Pacific\" to the df_ocean_processed DataFrame\n",
    "all_results['Ocean'] = 'Mediterranean'\n",
    "# save the data csv\n",
    "all_results.to_csv('par_means_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/2704579982.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5['Lat'] = df5['Lat'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/2704579982.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5['Long'] = df5['Long'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/2704579982.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5['Lat'] = df5['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/2704579982.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5['Long'] = df5['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# we will be only merge one ocean at a time \n",
    "\n",
    "df5 = Southern[['Lat','Long','Year']]\n",
    "# convert to 'Lat','Long' string \n",
    "df5['Lat'] = df5['Lat'].astype(str)\n",
    "df5['Long'] = df5['Long'].astype(str)\n",
    "\n",
    "# here shockingly we have chinese character in the data!\n",
    "df5['Lat'] = df5['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "df5['Long'] = df5['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "# drop nan in lat and long \n",
    "df5 = df5.dropna(subset=['Lat','Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the years\n",
    "years = range(1997, 2024)\n",
    "\n",
    "# Empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Get the original latitudes and longitudes\n",
    "orig_lats = df5['Lat'].copy()\n",
    "orig_lons = df5['Long'].copy()\n",
    "\n",
    "for year in years:\n",
    "    with tarfile.open(f'data/par.s.{year}.tar', \"r:\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Decide month from the filename=day of year\n",
    "                # last three integers\n",
    "                day_of_year = int(str(member.name.split('.')[1])[-3:])\n",
    "                month = str((day_of_year - 1) // 30 + 1).zfill(2)\n",
    "\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "                        # Access the 'npp' dataset\n",
    "                        data = hdf_file.select('par')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Define the latitude and longitude arrays\n",
    "                        lats = np.linspace(90, -90, data.shape[0])  # Shape[0] is the number of rows\n",
    "                        lons = np.linspace(-180, 180, data.shape[1])  # Shape[1] is the number of columns\n",
    "\n",
    "                        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "                        lat_list = lat_grid.reshape(-1,1)\n",
    "                        lon_list = lon_grid.reshape(-1,1)\n",
    "                        points = np.concatenate([lat_list, lon_list], axis=1)\n",
    "                        xi = np.array(list(zip(df5['Lat'], df5['Long'])))\n",
    "                        df5[f'par_{month}'] = griddata(points, data.ravel(), xi, method='nearest')\n",
    "\n",
    "        # Add the year column\n",
    "        df5['Year'] = year\n",
    "        df5['Lat'] = orig_lats\n",
    "        df5['Long'] = orig_lons\n",
    "\n",
    "        # Append df2 to the results DataFrame\n",
    "        all_results = pd.concat([all_results, df5])\n",
    "\n",
    "    # Close the file\n",
    "    hdf_file.end()\n",
    "\n",
    "# save the data\n",
    "all_results.to_csv('par_means_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of yearly mean\n",
    "all_results['yearly_mean_par'] = all_results.filter(regex='par_').mean(axis=1)\n",
    "# add a column of Cr_nmol/kg from the 'cleaned' excel file\n",
    "all_results['Cr_nmol/kg'] = Southern['Cr_nmol/kg']\n",
    "# create the 'Ocean' column filled with \"Pacific\" to the df_ocean_processed DataFrame\n",
    "all_results['Ocean'] = 'Southern'\n",
    "# save the data csv\n",
    "all_results.to_csv('par_means_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1638750860.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df6['Lat'] = df6['Lat'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1638750860.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df6['Long'] = df6['Long'].astype(str)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1638750860.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df6['Lat'] = df6['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
      "/var/folders/4f/0qgxggzd35j3d8dc232z56440000gn/T/ipykernel_20178/1638750860.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df6['Long'] = df6['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# we will be only merge one ocean at a time \n",
    "\n",
    "df6 = Arctic[['Lat','Long','Year']]\n",
    "# convert to 'Lat','Long' string \n",
    "df6['Lat'] = df6['Lat'].astype(str)\n",
    "df6['Long'] = df6['Long'].astype(str)\n",
    "\n",
    "# here shockingly we have chinese character in the data!\n",
    "df6['Lat'] = df6['Lat'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "df6['Long'] = df6['Long'].apply(lambda x: x.replace('−', '-')).astype(float)\n",
    "# drop nan in lat and long \n",
    "df6 = df6.dropna(subset=['Lat','Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the years\n",
    "years = range(1997, 2024)\n",
    "\n",
    "# Empty DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Get the original latitudes and longitudes\n",
    "orig_lats = df6['Lat'].copy()\n",
    "orig_lons = df6['Long'].copy()\n",
    "\n",
    "for year in years:\n",
    "    with tarfile.open(f'data/par.s.{year}.tar', \"r:\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Decide month from the filename=day of year\n",
    "                # last three integers\n",
    "                day_of_year = int(str(member.name.split('.')[1])[-3:])\n",
    "                month = str((day_of_year - 1) // 30 + 1).zfill(2)\n",
    "\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "                        # Access the 'npp' dataset\n",
    "                        data = hdf_file.select('par')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Define the latitude and longitude arrays\n",
    "                        lats = np.linspace(90, -90, data.shape[0])  # Shape[0] is the number of rows\n",
    "                        lons = np.linspace(-180, 180, data.shape[1])  # Shape[1] is the number of columns\n",
    "\n",
    "                        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "                        lat_list = lat_grid.reshape(-1,1)\n",
    "                        lon_list = lon_grid.reshape(-1,1)\n",
    "                        points = np.concatenate([lat_list, lon_list], axis=1)\n",
    "                        xi = np.array(list(zip(df6['Lat'], df6['Long'])))\n",
    "                        df6[f'par_{month}'] = griddata(points, data.ravel(), xi, method='nearest')\n",
    "\n",
    "        # Add the year column\n",
    "        df6['Year'] = year\n",
    "        df6['Lat'] = orig_lats\n",
    "        df6['Long'] = orig_lons\n",
    "\n",
    "        # Append df2 to the results DataFrame\n",
    "        all_results = pd.concat([all_results, df6])\n",
    "\n",
    "    # Close the file\n",
    "    hdf_file.end()\n",
    "\n",
    "# save the data\n",
    "all_results.to_csv('par_means_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of yearly mean\n",
    "all_results['yearly_mean_par'] = all_results.filter(regex='par_').mean(axis=1)\n",
    "# add a column of Cr_nmol/kg from the 'cleaned' excel file\n",
    "all_results['Cr_nmol/kg'] = Arctic['Cr_nmol/kg']\n",
    "# create the 'Ocean' column filled with \"Pacific\" to the df_ocean_processed DataFrame\n",
    "all_results['Ocean'] = 'Arctic'\n",
    "# save the data csv\n",
    "all_results.to_csv('par_means_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = pd.read_csv('par_means_1.csv')\n",
    "dt2 = pd.read_csv('par_means_2.csv')\n",
    "dt3 = pd.read_csv('par_means_3.csv')\n",
    "dt4 = pd.read_csv('par_means_4.csv')\n",
    "dt5 = pd.read_csv('par_means_5.csv')\n",
    "merged_df = pd.concat([dt1, dt2, dt3, dt4, dt5])\n",
    "merged_df.to_csv('par_means_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Year</th>\n",
       "      <th>par_10</th>\n",
       "      <th>par_11</th>\n",
       "      <th>par_12</th>\n",
       "      <th>par_01</th>\n",
       "      <th>par_02</th>\n",
       "      <th>par_04</th>\n",
       "      <th>par_05</th>\n",
       "      <th>par_06</th>\n",
       "      <th>par_07</th>\n",
       "      <th>par_08</th>\n",
       "      <th>par_09</th>\n",
       "      <th>par_03</th>\n",
       "      <th>yearly_mean_par</th>\n",
       "      <th>Cr_nmol/kg</th>\n",
       "      <th>Ocean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25595</th>\n",
       "      <td>-53.200000</td>\n",
       "      <td>-118.130000</td>\n",
       "      <td>2011</td>\n",
       "      <td>23.221207</td>\n",
       "      <td>33.755898</td>\n",
       "      <td>40.123245</td>\n",
       "      <td>38.709644</td>\n",
       "      <td>18.477451</td>\n",
       "      <td>12.326264</td>\n",
       "      <td>5.835998</td>\n",
       "      <td>3.655497</td>\n",
       "      <td>4.281707</td>\n",
       "      <td>10.785999</td>\n",
       "      <td>17.938750</td>\n",
       "      <td>21.155030</td>\n",
       "      <td>19.188890</td>\n",
       "      <td>3.610000</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24441</th>\n",
       "      <td>-53.200000</td>\n",
       "      <td>-118.130000</td>\n",
       "      <td>2003</td>\n",
       "      <td>29.996048</td>\n",
       "      <td>42.178066</td>\n",
       "      <td>45.970646</td>\n",
       "      <td>39.250920</td>\n",
       "      <td>22.219078</td>\n",
       "      <td>12.870097</td>\n",
       "      <td>6.004094</td>\n",
       "      <td>4.003114</td>\n",
       "      <td>4.463336</td>\n",
       "      <td>9.028788</td>\n",
       "      <td>15.881397</td>\n",
       "      <td>20.608252</td>\n",
       "      <td>21.039484</td>\n",
       "      <td>3.560000</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22030</th>\n",
       "      <td>12.588200</td>\n",
       "      <td>-17.572400</td>\n",
       "      <td>2021</td>\n",
       "      <td>41.133095</td>\n",
       "      <td>44.669000</td>\n",
       "      <td>41.831516</td>\n",
       "      <td>45.489967</td>\n",
       "      <td>55.651886</td>\n",
       "      <td>56.675297</td>\n",
       "      <td>55.496883</td>\n",
       "      <td>49.924030</td>\n",
       "      <td>48.094180</td>\n",
       "      <td>38.391980</td>\n",
       "      <td>48.538750</td>\n",
       "      <td>55.832714</td>\n",
       "      <td>48.477436</td>\n",
       "      <td>2.209834</td>\n",
       "      <td>Atlantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6959</th>\n",
       "      <td>-14.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>2009</td>\n",
       "      <td>49.378242</td>\n",
       "      <td>49.663567</td>\n",
       "      <td>52.415276</td>\n",
       "      <td>58.580124</td>\n",
       "      <td>51.227787</td>\n",
       "      <td>47.738200</td>\n",
       "      <td>40.054436</td>\n",
       "      <td>36.618750</td>\n",
       "      <td>38.746708</td>\n",
       "      <td>39.270485</td>\n",
       "      <td>46.376514</td>\n",
       "      <td>52.546062</td>\n",
       "      <td>46.884678</td>\n",
       "      <td>3.290217</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20144</th>\n",
       "      <td>54.500000</td>\n",
       "      <td>-48.460000</td>\n",
       "      <td>2015</td>\n",
       "      <td>10.800384</td>\n",
       "      <td>6.046431</td>\n",
       "      <td>2.643207</td>\n",
       "      <td>2.981126</td>\n",
       "      <td>16.395740</td>\n",
       "      <td>24.596098</td>\n",
       "      <td>33.646580</td>\n",
       "      <td>40.066520</td>\n",
       "      <td>40.962986</td>\n",
       "      <td>29.739094</td>\n",
       "      <td>20.107464</td>\n",
       "      <td>15.257352</td>\n",
       "      <td>20.270248</td>\n",
       "      <td>3.561254</td>\n",
       "      <td>Atlantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>24.167167</td>\n",
       "      <td>-160.253833</td>\n",
       "      <td>1997</td>\n",
       "      <td>42.051144</td>\n",
       "      <td>30.994768</td>\n",
       "      <td>30.020460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.355457</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9477</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>-106.000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>46.146694</td>\n",
       "      <td>32.922665</td>\n",
       "      <td>35.216774</td>\n",
       "      <td>38.659550</td>\n",
       "      <td>53.223454</td>\n",
       "      <td>56.529034</td>\n",
       "      <td>55.485367</td>\n",
       "      <td>50.392082</td>\n",
       "      <td>53.701256</td>\n",
       "      <td>53.961227</td>\n",
       "      <td>48.619278</td>\n",
       "      <td>51.740130</td>\n",
       "      <td>48.049790</td>\n",
       "      <td>2.290000</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25156</th>\n",
       "      <td>-54.830000</td>\n",
       "      <td>-95.680000</td>\n",
       "      <td>2008</td>\n",
       "      <td>25.020788</td>\n",
       "      <td>39.678770</td>\n",
       "      <td>34.635914</td>\n",
       "      <td>44.185596</td>\n",
       "      <td>32.300150</td>\n",
       "      <td>9.656915</td>\n",
       "      <td>5.138465</td>\n",
       "      <td>2.714197</td>\n",
       "      <td>4.511142</td>\n",
       "      <td>7.979820</td>\n",
       "      <td>18.968130</td>\n",
       "      <td>20.821610</td>\n",
       "      <td>20.467623</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10239</th>\n",
       "      <td>23.485000</td>\n",
       "      <td>-117.021667</td>\n",
       "      <td>2016</td>\n",
       "      <td>40.980595</td>\n",
       "      <td>33.403683</td>\n",
       "      <td>25.773417</td>\n",
       "      <td>35.674020</td>\n",
       "      <td>40.748740</td>\n",
       "      <td>52.316666</td>\n",
       "      <td>51.984970</td>\n",
       "      <td>48.914066</td>\n",
       "      <td>57.904350</td>\n",
       "      <td>48.172030</td>\n",
       "      <td>45.598133</td>\n",
       "      <td>45.837357</td>\n",
       "      <td>43.942337</td>\n",
       "      <td>2.698357</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>24.698333</td>\n",
       "      <td>-114.106667</td>\n",
       "      <td>2004</td>\n",
       "      <td>43.194904</td>\n",
       "      <td>34.818195</td>\n",
       "      <td>28.147224</td>\n",
       "      <td>31.711159</td>\n",
       "      <td>40.731240</td>\n",
       "      <td>48.593280</td>\n",
       "      <td>56.947790</td>\n",
       "      <td>55.246437</td>\n",
       "      <td>54.605484</td>\n",
       "      <td>56.805710</td>\n",
       "      <td>48.676662</td>\n",
       "      <td>42.518837</td>\n",
       "      <td>45.166412</td>\n",
       "      <td>2.950474</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Lat        Long  Year     par_10     par_11     par_12  \\\n",
       "25595 -53.200000 -118.130000  2011  23.221207  33.755898  40.123245   \n",
       "24441 -53.200000 -118.130000  2003  29.996048  42.178066  45.970646   \n",
       "22030  12.588200  -17.572400  2021  41.133095  44.669000  41.831516   \n",
       "6959  -14.000000  -99.000000  2009  49.378242  49.663567  52.415276   \n",
       "20144  54.500000  -48.460000  2015  10.800384   6.046431   2.643207   \n",
       "276    24.167167 -160.253833  1997  42.051144  30.994768  30.020460   \n",
       "9477   20.000000 -106.000000  2014  46.146694  32.922665  35.216774   \n",
       "25156 -54.830000  -95.680000  2008  25.020788  39.678770  34.635914   \n",
       "10239  23.485000 -117.021667  2016  40.980595  33.403683  25.773417   \n",
       "3845   24.698333 -114.106667  2004  43.194904  34.818195  28.147224   \n",
       "\n",
       "          par_01     par_02     par_04     par_05     par_06     par_07  \\\n",
       "25595  38.709644  18.477451  12.326264   5.835998   3.655497   4.281707   \n",
       "24441  39.250920  22.219078  12.870097   6.004094   4.003114   4.463336   \n",
       "22030  45.489967  55.651886  56.675297  55.496883  49.924030  48.094180   \n",
       "6959   58.580124  51.227787  47.738200  40.054436  36.618750  38.746708   \n",
       "20144   2.981126  16.395740  24.596098  33.646580  40.066520  40.962986   \n",
       "276          NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "9477   38.659550  53.223454  56.529034  55.485367  50.392082  53.701256   \n",
       "25156  44.185596  32.300150   9.656915   5.138465   2.714197   4.511142   \n",
       "10239  35.674020  40.748740  52.316666  51.984970  48.914066  57.904350   \n",
       "3845   31.711159  40.731240  48.593280  56.947790  55.246437  54.605484   \n",
       "\n",
       "          par_08     par_09     par_03  yearly_mean_par  Cr_nmol/kg     Ocean  \n",
       "25595  10.785999  17.938750  21.155030        19.188890    3.610000  Southern  \n",
       "24441   9.028788  15.881397  20.608252        21.039484    3.560000  Southern  \n",
       "22030  38.391980  48.538750  55.832714        48.477436    2.209834  Atlantic  \n",
       "6959   39.270485  46.376514  52.546062        46.884678    3.290217   Pacific  \n",
       "20144  29.739094  20.107464  15.257352        20.270248    3.561254  Atlantic  \n",
       "276          NaN        NaN        NaN        34.355457    2.980000   Pacific  \n",
       "9477   53.961227  48.619278  51.740130        48.049790    2.290000   Pacific  \n",
       "25156   7.979820  18.968130  20.821610        20.467623    3.580000  Southern  \n",
       "10239  48.172030  45.598133  45.837357        43.942337    2.698357   Pacific  \n",
       "3845   56.805710  48.676662  42.518837        45.166412    2.950474   Pacific  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the data\n",
    "df_par_means = pd.read_csv('par_means_new.csv')\n",
    "df_par_means.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
