{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step one download the file \n",
    "### change your url \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data from SeaWiFS: http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.s.chl.a.sst.php\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# make sure 'data/' directory exists\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')\n",
    "\n",
    "#############################################################################################################\n",
    "##your code#################################################################################################\n",
    "#############################################################################################################\n",
    "base_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/mld030.hycom/hdf/mld.hycom_030.{}.tar'\n",
    "\n",
    "for year in range(1997, 2024):  # 2010 is not included\n",
    "    url = base_url.format(year)\n",
    "    filename = f'data/mld.hycom_030.{year}.tar'\n",
    "    download_file(url, filename)\n",
    "\n",
    "\n",
    "# download the data from MODIS:  http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.m.chl.m.sst.php\n",
    "\n",
    "#############################################################################################################\n",
    "##your code#################################################################################################\n",
    "#############################################################################################################\n",
    "# new_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/vgpm.r2022.m.chl.m.sst/hdf/vgpm.m.{}.tar'\n",
    "\n",
    "# for year in range(2010, 2024):  # 2024 is not included\n",
    "#     url = new_url.format(year)\n",
    "#     filename = f'data/vgpm.s.{year}.tar'\n",
    "#     download_file(url, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step two calculate the mean of whole data\n",
    "### please check your data and the final dataset , name it properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import tempfile\n",
    "from pyhdf.SD import SD, SDC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the latitude and longitude arrays\n",
    "lats = np.linspace(90, -90, 1080)\n",
    "lons = np.linspace(-180, 180, 2160)\n",
    "\n",
    "# Create a meshgrid of latitudes and longitudes\n",
    "lon_grid, lat_grid = np.meshgrid(lons, lats, indexing='ij')\n",
    "\n",
    "# Flatten the arrays for dataframe construction\n",
    "lat_flat = lat_grid.flatten()\n",
    "lon_flat = lon_grid.flatten()\n",
    "\n",
    "# Create a multi-index from the latitude and longitude arrays\n",
    "index = pd.MultiIndex.from_arrays([lat_flat, lon_flat], names=['Lat', 'Long'])\n",
    "\n",
    "# Create an empty DataFrame with this index\n",
    "df = pd.DataFrame(index=index)\n",
    "\n",
    "# Loop through each tar file\n",
    "for year in range(1997, 2024):\n",
    "    # Open the tar file\n",
    "    tar_filename = f'data/mld.hycom_030.{year}.tar'\n",
    "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
    "        \n",
    "        # Create an empty DataFrame to store data for this year\n",
    "        temp_df = pd.DataFrame(index=index)\n",
    "\n",
    "        # Loop through each member of the tar file\n",
    "        for member in tar.getmembers():\n",
    "            # If it's a .hdf.gz file\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "###your code#################################################################################################\n",
    "### change 'mld' to '' ##################################################################################\n",
    "                        # Access the 'mld' dataset\n",
    "                        data = hdf_file.select('mld')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Flatten the data and add it to the temporary DataFrame\n",
    "                        data_flat = data.flatten()\n",
    "                        temp_df[member.name] = data_flat\n",
    "\n",
    "                        # Close the file\n",
    "                        hdf_file.end()\n",
    "\n",
    "        # After going through all files for the year, calculate the mean for each location (ignoring NaNs)\n",
    "        mean_data = temp_df.mean(axis=1, skipna=True)\n",
    "\n",
    "        # Add this DataFrame to the main DataFrame\n",
    "        df[str(year)] = mean_data\n",
    "\n",
    "# Reset the index of the DataFrame, making 'Lat' and 'Long' normal columns\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('mld_means.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.826600</td>\n",
       "      <td>27.211823</td>\n",
       "      <td>32.903030</td>\n",
       "      <td>32.963802</td>\n",
       "      <td>32.608980</td>\n",
       "      <td>46.673367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.833179</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.837618</td>\n",
       "      <td>27.215448</td>\n",
       "      <td>32.909860</td>\n",
       "      <td>32.967907</td>\n",
       "      <td>32.610250</td>\n",
       "      <td>46.677254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.666358</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.868065</td>\n",
       "      <td>27.213402</td>\n",
       "      <td>32.911903</td>\n",
       "      <td>32.970306</td>\n",
       "      <td>32.611500</td>\n",
       "      <td>46.679893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.499537</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.875278</td>\n",
       "      <td>27.214859</td>\n",
       "      <td>32.917380</td>\n",
       "      <td>32.973010</td>\n",
       "      <td>32.611800</td>\n",
       "      <td>46.676773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.332715</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.876595</td>\n",
       "      <td>27.214561</td>\n",
       "      <td>32.918095</td>\n",
       "      <td>32.979294</td>\n",
       "      <td>32.613052</td>\n",
       "      <td>46.680485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lat   Long  1997  1998  1999  2000  2001  2002  2003  2004  ...   \n",
       "0  90.000000 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...  \\\n",
       "1  89.833179 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2  89.666358 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3  89.499537 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4  89.332715 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "   2014  2015  2016  2017       2018       2019       2020       2021   \n",
       "0   NaN   NaN   NaN   NaN  30.826600  27.211823  32.903030  32.963802  \\\n",
       "1   NaN   NaN   NaN   NaN  30.837618  27.215448  32.909860  32.967907   \n",
       "2   NaN   NaN   NaN   NaN  30.868065  27.213402  32.911903  32.970306   \n",
       "3   NaN   NaN   NaN   NaN  30.875278  27.214859  32.917380  32.973010   \n",
       "4   NaN   NaN   NaN   NaN  30.876595  27.214561  32.918095  32.979294   \n",
       "\n",
       "        2022       2023  \n",
       "0  32.608980  46.673367  \n",
       "1  32.610250  46.677254  \n",
       "2  32.611500  46.679893  \n",
       "3  32.611800  46.676773  \n",
       "4  32.613052  46.680485  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mld_means = pd.read_csv('mld_means.csv')\n",
    "df_mld_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.332800e+06</td>\n",
       "      <td>2.332800e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>1.408276e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.402017e+06</td>\n",
       "      <td>1.402017e+06</td>\n",
       "      <td>1.402017e+06</td>\n",
       "      <td>1.402017e+06</td>\n",
       "      <td>1.535387e+06</td>\n",
       "      <td>1.533697e+06</td>\n",
       "      <td>1.533697e+06</td>\n",
       "      <td>1.533697e+06</td>\n",
       "      <td>1.533697e+06</td>\n",
       "      <td>1.533697e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-6.736998e-15</td>\n",
       "      <td>-2.410972e-15</td>\n",
       "      <td>6.149137e+01</td>\n",
       "      <td>6.294367e+01</td>\n",
       "      <td>6.257512e+01</td>\n",
       "      <td>6.286864e+01</td>\n",
       "      <td>6.066013e+01</td>\n",
       "      <td>6.026777e+01</td>\n",
       "      <td>5.962403e+01</td>\n",
       "      <td>5.992304e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.004174e+01</td>\n",
       "      <td>6.228459e+01</td>\n",
       "      <td>6.103743e+01</td>\n",
       "      <td>6.081088e+01</td>\n",
       "      <td>5.912681e+01</td>\n",
       "      <td>4.854502e+01</td>\n",
       "      <td>4.774283e+01</td>\n",
       "      <td>4.881339e+01</td>\n",
       "      <td>4.879551e+01</td>\n",
       "      <td>4.675138e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.200967e+01</td>\n",
       "      <td>1.039712e+02</td>\n",
       "      <td>4.097085e+01</td>\n",
       "      <td>3.778995e+01</td>\n",
       "      <td>3.794584e+01</td>\n",
       "      <td>3.849731e+01</td>\n",
       "      <td>3.607759e+01</td>\n",
       "      <td>3.540604e+01</td>\n",
       "      <td>3.565189e+01</td>\n",
       "      <td>3.603428e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.744433e+01</td>\n",
       "      <td>4.031214e+01</td>\n",
       "      <td>3.817451e+01</td>\n",
       "      <td>3.758844e+01</td>\n",
       "      <td>3.664786e+01</td>\n",
       "      <td>2.655985e+01</td>\n",
       "      <td>2.688019e+01</td>\n",
       "      <td>2.850043e+01</td>\n",
       "      <td>2.807238e+01</td>\n",
       "      <td>3.471590e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.000000e+01</td>\n",
       "      <td>-1.800000e+02</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.993215e+00</td>\n",
       "      <td>3.986439e+00</td>\n",
       "      <td>3.990579e+00</td>\n",
       "      <td>3.995166e+00</td>\n",
       "      <td>3.999180e+00</td>\n",
       "      <td>3.988292e+00</td>\n",
       "      <td>3.998876e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>9.497684e+00</td>\n",
       "      <td>9.514389e+00</td>\n",
       "      <td>9.511917e+00</td>\n",
       "      <td>9.517311e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.500000e+01</td>\n",
       "      <td>-9.000000e+01</td>\n",
       "      <td>3.582031e+01</td>\n",
       "      <td>3.866857e+01</td>\n",
       "      <td>3.813681e+01</td>\n",
       "      <td>3.819613e+01</td>\n",
       "      <td>3.740963e+01</td>\n",
       "      <td>3.743136e+01</td>\n",
       "      <td>3.689331e+01</td>\n",
       "      <td>3.671695e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.368140e+01</td>\n",
       "      <td>3.425556e+01</td>\n",
       "      <td>3.357554e+01</td>\n",
       "      <td>3.404565e+01</td>\n",
       "      <td>3.408250e+01</td>\n",
       "      <td>3.122709e+01</td>\n",
       "      <td>3.079055e+01</td>\n",
       "      <td>3.104953e+01</td>\n",
       "      <td>3.116683e+01</td>\n",
       "      <td>2.690473e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-7.105427e-15</td>\n",
       "      <td>-1.425249e-14</td>\n",
       "      <td>4.695459e+01</td>\n",
       "      <td>5.060216e+01</td>\n",
       "      <td>5.073407e+01</td>\n",
       "      <td>5.067403e+01</td>\n",
       "      <td>4.986629e+01</td>\n",
       "      <td>4.927116e+01</td>\n",
       "      <td>4.845621e+01</td>\n",
       "      <td>4.885675e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.810646e+01</td>\n",
       "      <td>4.790160e+01</td>\n",
       "      <td>4.807678e+01</td>\n",
       "      <td>4.841512e+01</td>\n",
       "      <td>4.712770e+01</td>\n",
       "      <td>4.159988e+01</td>\n",
       "      <td>4.084117e+01</td>\n",
       "      <td>4.187149e+01</td>\n",
       "      <td>4.136847e+01</td>\n",
       "      <td>3.966084e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>7.562315e+01</td>\n",
       "      <td>7.932338e+01</td>\n",
       "      <td>7.681049e+01</td>\n",
       "      <td>7.733212e+01</td>\n",
       "      <td>7.456791e+01</td>\n",
       "      <td>7.482238e+01</td>\n",
       "      <td>7.357315e+01</td>\n",
       "      <td>7.408436e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>7.698699e+01</td>\n",
       "      <td>8.040919e+01</td>\n",
       "      <td>8.108750e+01</td>\n",
       "      <td>7.960258e+01</td>\n",
       "      <td>7.565919e+01</td>\n",
       "      <td>5.658365e+01</td>\n",
       "      <td>5.530078e+01</td>\n",
       "      <td>5.597609e+01</td>\n",
       "      <td>5.760262e+01</td>\n",
       "      <td>5.625160e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>3.771343e+02</td>\n",
       "      <td>3.795920e+02</td>\n",
       "      <td>3.788337e+02</td>\n",
       "      <td>3.790154e+02</td>\n",
       "      <td>3.768887e+02</td>\n",
       "      <td>3.798910e+02</td>\n",
       "      <td>3.769129e+02</td>\n",
       "      <td>3.800199e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.858532e+02</td>\n",
       "      <td>3.848569e+02</td>\n",
       "      <td>3.848225e+02</td>\n",
       "      <td>3.849182e+02</td>\n",
       "      <td>3.827087e+02</td>\n",
       "      <td>3.919591e+02</td>\n",
       "      <td>3.307609e+02</td>\n",
       "      <td>2.284897e+02</td>\n",
       "      <td>2.377766e+02</td>\n",
       "      <td>3.925975e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Lat          Long          1997          1998          1999   \n",
       "count  2.332800e+06  2.332800e+06  1.408276e+06  1.408276e+06  1.408276e+06  \\\n",
       "mean  -6.736998e-15 -2.410972e-15  6.149137e+01  6.294367e+01  6.257512e+01   \n",
       "std    5.200967e+01  1.039712e+02  4.097085e+01  3.778995e+01  3.794584e+01   \n",
       "min   -9.000000e+01 -1.800000e+02  4.000000e+00  3.993215e+00  3.986439e+00   \n",
       "25%   -4.500000e+01 -9.000000e+01  3.582031e+01  3.866857e+01  3.813681e+01   \n",
       "50%   -7.105427e-15 -1.425249e-14  4.695459e+01  5.060216e+01  5.073407e+01   \n",
       "75%    4.500000e+01  9.000000e+01  7.562315e+01  7.932338e+01  7.681049e+01   \n",
       "max    9.000000e+01  1.800000e+02  3.771343e+02  3.795920e+02  3.788337e+02   \n",
       "\n",
       "               2000          2001          2002          2003          2004   \n",
       "count  1.408276e+06  1.408276e+06  1.408276e+06  1.408276e+06  1.408276e+06  \\\n",
       "mean   6.286864e+01  6.066013e+01  6.026777e+01  5.962403e+01  5.992304e+01   \n",
       "std    3.849731e+01  3.607759e+01  3.540604e+01  3.565189e+01  3.603428e+01   \n",
       "min    3.990579e+00  3.995166e+00  3.999180e+00  3.988292e+00  3.998876e+00   \n",
       "25%    3.819613e+01  3.740963e+01  3.743136e+01  3.689331e+01  3.671695e+01   \n",
       "50%    5.067403e+01  4.986629e+01  4.927116e+01  4.845621e+01  4.885675e+01   \n",
       "75%    7.733212e+01  7.456791e+01  7.482238e+01  7.357315e+01  7.408436e+01   \n",
       "max    3.790154e+02  3.768887e+02  3.798910e+02  3.769129e+02  3.800199e+02   \n",
       "\n",
       "       ...          2014          2015          2016          2017   \n",
       "count  ...  1.402017e+06  1.402017e+06  1.402017e+06  1.402017e+06  \\\n",
       "mean   ...  6.004174e+01  6.228459e+01  6.103743e+01  6.081088e+01   \n",
       "std    ...  3.744433e+01  4.031214e+01  3.817451e+01  3.758844e+01   \n",
       "min    ...  9.497684e+00  9.514389e+00  9.511917e+00  9.517311e+00   \n",
       "25%    ...  3.368140e+01  3.425556e+01  3.357554e+01  3.404565e+01   \n",
       "50%    ...  4.810646e+01  4.790160e+01  4.807678e+01  4.841512e+01   \n",
       "75%    ...  7.698699e+01  8.040919e+01  8.108750e+01  7.960258e+01   \n",
       "max    ...  3.858532e+02  3.848569e+02  3.848225e+02  3.849182e+02   \n",
       "\n",
       "               2018          2019          2020          2021          2022   \n",
       "count  1.535387e+06  1.533697e+06  1.533697e+06  1.533697e+06  1.533697e+06  \\\n",
       "mean   5.912681e+01  4.854502e+01  4.774283e+01  4.881339e+01  4.879551e+01   \n",
       "std    3.664786e+01  2.655985e+01  2.688019e+01  2.850043e+01  2.807238e+01   \n",
       "min    4.000000e+00  4.000000e+00  4.000000e+00  4.000000e+00  4.000000e+00   \n",
       "25%    3.408250e+01  3.122709e+01  3.079055e+01  3.104953e+01  3.116683e+01   \n",
       "50%    4.712770e+01  4.159988e+01  4.084117e+01  4.187149e+01  4.136847e+01   \n",
       "75%    7.565919e+01  5.658365e+01  5.530078e+01  5.597609e+01  5.760262e+01   \n",
       "max    3.827087e+02  3.919591e+02  3.307609e+02  2.284897e+02  2.377766e+02   \n",
       "\n",
       "               2023  \n",
       "count  1.533697e+06  \n",
       "mean   4.675138e+01  \n",
       "std    3.471590e+01  \n",
       "min    4.000000e+00  \n",
       "25%    2.690473e+01  \n",
       "50%    3.966084e+01  \n",
       "75%    5.625160e+01  \n",
       "max    3.925975e+02  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mld_means.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three read cleaned excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> read cleadned excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read new data\n",
    "# read each page from excel file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filename='cleaned'\n",
    "Pacific=pd.read_excel(filename+'.xlsx',sheet_name='Pacific')\n",
    "Atlantic=pd.read_excel(filename+'.xlsx',sheet_name='Atlantic')\n",
    "Mediterranean=pd.read_excel(filename+'.xlsx',sheet_name='Mediterranean')\n",
    "Southern=pd.read_excel(filename+'.xlsx',sheet_name='Southern Ocean')\n",
    "Arctic=pd.read_excel(filename+'.xlsx',sheet_name='Arctic')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step four melt for merge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> melt for merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the data\n",
    "#############################################################################################################\n",
    "##your code#################################################################################################\n",
    "# df_npp_melt = df_npp_means_new.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='npp')\n",
    "# df_chl_melt = df_chl_means_new.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='chl')\n",
    "# df_par_melt = df_par_means_new.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='par')\n",
    "# df_sst_melt = df_sst_means_new.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='sst')\n",
    "df_mld_melt = df_mld_means.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='mld')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step five Select decimal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then merge on these columns\n",
    "# assuming df_melted_variable1-5 and ocean_dfs are your dataframes\n",
    "ocean_names = [\"Pacific\", \"Atlantic\", \"Mediterranean\", \"Southern\", \"Arctic\"]\n",
    "# df_melted_list = [df_npp_melt, df_chl_melt, df_par_melt, df_sst_melt, df_mld_melt]\n",
    "#############################################################################################################\n",
    "##your code#################################################################################################\n",
    "#############################################################################################################\n",
    "\n",
    "# change this to your dataframes\n",
    "df_melted_list = [df_mld_melt]\n",
    "ocean_dfs = [Pacific, Atlantic, Mediterranean, Southern, Arctic]\n",
    "\n",
    "\n",
    "for df in df_melted_list + ocean_dfs:\n",
    "    # convert types safely\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
    "    df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "    df['Long'] = pd.to_numeric(df['Long'], errors='coerce')\n",
    "\n",
    "\n",
    "# round lat and long to 0.0001\n",
    "# I believe round is a way of thresholding the data, let me know if there is a better wayof matching \n",
    "# the lat and longs\n",
    "for df in df_melted_list + ocean_dfs:\n",
    "    df['Lat'] = df['Lat'].round(1)\n",
    "    df['Long'] = df['Long'].round(1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check if there is a match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of tuples for the Pacific dataframe\n",
    "pacific_coords = set(zip(Pacific['Lat'], Pacific['Long']))\n",
    "\n",
    "# Create a set of tuples for the df_npp dataframe\n",
    "npp_coords = set(zip(df_mld_melt['Lat'], df_mld_melt['Long']))\n",
    "\n",
    "# Find the common coordinates\n",
    "common_coords = pacific_coords.intersection(npp_coords)\n",
    "\n",
    "print(f\"Common coordinates between Pacific and df_npp: {len(common_coords)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step six merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "merged_dfs = []  # List to store merged dataframes\n",
    "\n",
    "for ocean_name, ocean_df in zip(ocean_names, ocean_dfs):\n",
    "    # merge ocean_df with all dataframes in df_melted_list\n",
    "    for df_melted in df_melted_list:\n",
    "        ocean_df = pd.merge(ocean_df, df_melted, on=['Lat', 'Long', 'Year'], how='left')\n",
    "\n",
    "    # Fill NaN values with the mean of each column\n",
    "    for col in ocean_df.select_dtypes(include=[np.number]).columns:\n",
    "        ocean_df[col] = ocean_df[col].astype(float)\n",
    "        ocean_df[col].fillna(ocean_df[col].mean(), inplace=True)\n",
    "        if ocean_df[col].apply(float.is_integer).all():  # Check if all values are integer\n",
    "            ocean_df[col] = ocean_df[col].astype('Int64')  # Change dtype back to integer\n",
    "\n",
    "    merged_dfs.append(ocean_df)  # Append the merged dataframe to the list\n",
    "\n",
    "with pd.ExcelWriter('merged.xlsx') as writer:\n",
    "    for ocean_name, merged_df in zip(ocean_names, merged_dfs):\n",
    "        merged_df.to_excel(writer, sheet_name=ocean_name)  # Write each merged dataframe to a different sheet\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
