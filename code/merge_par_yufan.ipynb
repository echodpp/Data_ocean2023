{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeaWIFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/par.s.1997.tar' already exists. Skipping download.\n",
      "File 'data/par.s.1998.tar' already exists. Skipping download.\n",
      "File 'data/par.s.1999.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2000.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2001.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2002.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2003.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2004.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2005.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2006.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2007.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2008.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2009.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2010.tar' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# make sure 'data/' directory exists\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')\n",
    "\n",
    "base_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/par.r2014.seawifs/hdf/par.s.{}.tar'\n",
    "\n",
    "for year in range(1997, 2011):  # 2011 is not included\n",
    "    url = base_url.format(year)\n",
    "    filename = f'data/par.s.{year}.tar'\n",
    "    download_file(url, filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step two: calculate the mean of whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... ERROR conda.notices.fetch:get_channel_notice_response(67): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/main/notices.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))> for channel: defaults url: https://repo.anaconda.com/pkgs/main/notices.json\n",
      "ERROR conda.notices.fetch:get_channel_notice_response(67): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/r/notices.json (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))> for channel: defaults url: https://repo.anaconda.com/pkgs/r/notices.json\n",
      "done\n",
      "Collecting package metadata (current_repodata.json): failed\n",
      "\n",
      "ProxyError: Conda cannot proceed due to an error in your proxy configuration.\n",
      "Check for typos and other configuration errors in any '.netrc' file in your home directory,\n",
      "any environment variables ending in '_PROXY', and any other system-wide proxy\n",
      "configuration settings.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda update numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): failed\n",
      "\n",
      "ProxyError: Conda cannot proceed due to an error in your proxy configuration.\n",
      "Check for typos and other configuration errors in any '.netrc' file in your home directory,\n",
      "any environment variables ending in '_PROXY', and any other system-wide proxy\n",
      "configuration settings.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda update pyhdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyhdf.SD import SD, SDC\n",
    "\n",
    "# Define the latitude and longitude arrays\n",
    "lats = np.linspace(90, -90, 1080)\n",
    "lons = np.linspace(-180, 180, 2160)\n",
    "\n",
    "# Create a meshgrid of latitudes and longitudes\n",
    "lon_grid, lat_grid = np.meshgrid(lons, lats, indexing='ij')\n",
    "\n",
    "# Flatten the arrays for dataframe construction\n",
    "lat_flat = lat_grid.flatten()\n",
    "lon_flat = lon_grid.flatten()\n",
    "\n",
    "# Create a multi-index from the latitude and longitude arrays\n",
    "index = pd.MultiIndex.from_arrays([lat_flat, lon_flat], names=['Lat', 'Long'])\n",
    "\n",
    "# Create an empty DataFrame with this index\n",
    "df = pd.DataFrame(index=index)\n",
    "\n",
    "# Loop through each tar file\n",
    "for year in range(1997, 2011):\n",
    "    # Open the tar file\n",
    "    tar_filename = f'data/par.s.{year}.tar'\n",
    "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
    "\n",
    "         # Create an empty DataFrame to store data for this year\n",
    "        temp_df = pd.DataFrame(index=index)\n",
    "\n",
    "        # Loop through each member of the tar file\n",
    "        for member in tar.getmembers():\n",
    "            # If it's a .hdf.gz file\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "\n",
    "                        # Access the 'npp' dataset\n",
    "                        data = hdf_file.select('par')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Flatten the data and add it to the temporary DataFrame\n",
    "                        data_flat = data.flatten()\n",
    "                        temp_df[member.name] = data_flat\n",
    "\n",
    "                        # Close the file\n",
    "                        hdf_file.end()\n",
    "\n",
    "        # After going through all files for the year, calculate the mean for each location (ignoring NaNs)\n",
    "        mean_data = temp_df.mean(axis=1, skipna=True)\n",
    "\n",
    "        # Add this DataFrame to the main DataFrame\n",
    "        df[str(year)] = mean_data\n",
    "\n",
    "# Reset the index of the DataFrame, making 'Lat' and 'Long' normal columns\n",
    "df.reset_index(inplace=True)\n",
    "                      \n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('par_means_s.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.833179</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.666358</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.499537</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.332715</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lat   Long  1997  1998  1999  2000  2001  2002  2003  2004  2005  \\\n",
       "0  90.000000 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1  89.833179 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2  89.666358 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3  89.499537 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4  89.332715 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   2006  2007  2008  2009  2010  \n",
       "0   NaN   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN   NaN  \n",
       "2   NaN   NaN   NaN   NaN   NaN  \n",
       "3   NaN   NaN   NaN   NaN   NaN  \n",
       "4   NaN   NaN   NaN   NaN   NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_par_means = pd.read_csv('par_means_s.csv')\n",
    "df_par_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.332800e+06</td>\n",
       "      <td>2.332800e+06</td>\n",
       "      <td>1.328358e+06</td>\n",
       "      <td>1.431556e+06</td>\n",
       "      <td>1.434942e+06</td>\n",
       "      <td>1.433280e+06</td>\n",
       "      <td>1.428070e+06</td>\n",
       "      <td>1.525700e+06</td>\n",
       "      <td>1.559677e+06</td>\n",
       "      <td>1.564153e+06</td>\n",
       "      <td>1.571626e+06</td>\n",
       "      <td>1.570545e+06</td>\n",
       "      <td>1.587869e+06</td>\n",
       "      <td>1.579729e+06</td>\n",
       "      <td>1.573119e+06</td>\n",
       "      <td>1.581705e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.347462e-14</td>\n",
       "      <td>5.988442e-16</td>\n",
       "      <td>3.648680e+01</td>\n",
       "      <td>3.293971e+01</td>\n",
       "      <td>3.286141e+01</td>\n",
       "      <td>3.272164e+01</td>\n",
       "      <td>3.257054e+01</td>\n",
       "      <td>3.404737e+01</td>\n",
       "      <td>3.359226e+01</td>\n",
       "      <td>3.340154e+01</td>\n",
       "      <td>3.317612e+01</td>\n",
       "      <td>3.329809e+01</td>\n",
       "      <td>3.287351e+01</td>\n",
       "      <td>3.321420e+01</td>\n",
       "      <td>3.320171e+01</td>\n",
       "      <td>3.301338e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.200967e+01</td>\n",
       "      <td>1.039712e+02</td>\n",
       "      <td>1.437401e+01</td>\n",
       "      <td>1.081675e+01</td>\n",
       "      <td>1.097235e+01</td>\n",
       "      <td>1.110469e+01</td>\n",
       "      <td>1.137511e+01</td>\n",
       "      <td>1.146245e+01</td>\n",
       "      <td>1.167125e+01</td>\n",
       "      <td>1.173296e+01</td>\n",
       "      <td>1.186311e+01</td>\n",
       "      <td>1.179777e+01</td>\n",
       "      <td>1.205750e+01</td>\n",
       "      <td>1.193083e+01</td>\n",
       "      <td>1.184485e+01</td>\n",
       "      <td>1.192258e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.000000e+01</td>\n",
       "      <td>-1.800000e+02</td>\n",
       "      <td>1.999000e-03</td>\n",
       "      <td>1.199300e-02</td>\n",
       "      <td>2.119980e-01</td>\n",
       "      <td>2.999900e-02</td>\n",
       "      <td>1.199300e-02</td>\n",
       "      <td>3.266356e-02</td>\n",
       "      <td>5.799689e-02</td>\n",
       "      <td>7.399689e-02</td>\n",
       "      <td>6.799689e-02</td>\n",
       "      <td>7.899690e-02</td>\n",
       "      <td>4.599689e-02</td>\n",
       "      <td>1.459969e-01</td>\n",
       "      <td>1.899689e-02</td>\n",
       "      <td>9.199689e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.500000e+01</td>\n",
       "      <td>-9.000000e+01</td>\n",
       "      <td>2.955235e+01</td>\n",
       "      <td>2.242377e+01</td>\n",
       "      <td>2.230057e+01</td>\n",
       "      <td>2.197562e+01</td>\n",
       "      <td>2.159606e+01</td>\n",
       "      <td>2.379849e+01</td>\n",
       "      <td>2.268313e+01</td>\n",
       "      <td>2.230756e+01</td>\n",
       "      <td>2.221823e+01</td>\n",
       "      <td>2.210608e+01</td>\n",
       "      <td>2.171053e+01</td>\n",
       "      <td>2.209200e+01</td>\n",
       "      <td>2.220668e+01</td>\n",
       "      <td>2.207667e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.105427e-15</td>\n",
       "      <td>-1.425249e-14</td>\n",
       "      <td>3.971651e+01</td>\n",
       "      <td>3.352026e+01</td>\n",
       "      <td>3.345121e+01</td>\n",
       "      <td>3.349470e+01</td>\n",
       "      <td>3.371332e+01</td>\n",
       "      <td>3.578067e+01</td>\n",
       "      <td>3.460883e+01</td>\n",
       "      <td>3.459970e+01</td>\n",
       "      <td>3.409866e+01</td>\n",
       "      <td>3.417763e+01</td>\n",
       "      <td>3.401879e+01</td>\n",
       "      <td>3.436450e+01</td>\n",
       "      <td>3.401798e+01</td>\n",
       "      <td>3.408301e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>4.783021e+01</td>\n",
       "      <td>4.351616e+01</td>\n",
       "      <td>4.318112e+01</td>\n",
       "      <td>4.327298e+01</td>\n",
       "      <td>4.326261e+01</td>\n",
       "      <td>4.390462e+01</td>\n",
       "      <td>4.435869e+01</td>\n",
       "      <td>4.429033e+01</td>\n",
       "      <td>4.407848e+01</td>\n",
       "      <td>4.423293e+01</td>\n",
       "      <td>4.395175e+01</td>\n",
       "      <td>4.411874e+01</td>\n",
       "      <td>4.414447e+01</td>\n",
       "      <td>4.400641e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>6.624800e+01</td>\n",
       "      <td>6.409000e+01</td>\n",
       "      <td>6.307400e+01</td>\n",
       "      <td>6.310400e+01</td>\n",
       "      <td>6.422200e+01</td>\n",
       "      <td>6.720000e+01</td>\n",
       "      <td>6.812823e+01</td>\n",
       "      <td>6.927200e+01</td>\n",
       "      <td>6.789000e+01</td>\n",
       "      <td>6.712600e+01</td>\n",
       "      <td>6.914400e+01</td>\n",
       "      <td>6.941600e+01</td>\n",
       "      <td>6.787400e+01</td>\n",
       "      <td>6.725800e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Lat          Long          1997          1998          1999  \\\n",
       "count  2.332800e+06  2.332800e+06  1.328358e+06  1.431556e+06  1.434942e+06   \n",
       "mean   1.347462e-14  5.988442e-16  3.648680e+01  3.293971e+01  3.286141e+01   \n",
       "std    5.200967e+01  1.039712e+02  1.437401e+01  1.081675e+01  1.097235e+01   \n",
       "min   -9.000000e+01 -1.800000e+02  1.999000e-03  1.199300e-02  2.119980e-01   \n",
       "25%   -4.500000e+01 -9.000000e+01  2.955235e+01  2.242377e+01  2.230057e+01   \n",
       "50%    7.105427e-15 -1.425249e-14  3.971651e+01  3.352026e+01  3.345121e+01   \n",
       "75%    4.500000e+01  9.000000e+01  4.783021e+01  4.351616e+01  4.318112e+01   \n",
       "max    9.000000e+01  1.800000e+02  6.624800e+01  6.409000e+01  6.307400e+01   \n",
       "\n",
       "               2000          2001          2002          2003          2004  \\\n",
       "count  1.433280e+06  1.428070e+06  1.525700e+06  1.559677e+06  1.564153e+06   \n",
       "mean   3.272164e+01  3.257054e+01  3.404737e+01  3.359226e+01  3.340154e+01   \n",
       "std    1.110469e+01  1.137511e+01  1.146245e+01  1.167125e+01  1.173296e+01   \n",
       "min    2.999900e-02  1.199300e-02  3.266356e-02  5.799689e-02  7.399689e-02   \n",
       "25%    2.197562e+01  2.159606e+01  2.379849e+01  2.268313e+01  2.230756e+01   \n",
       "50%    3.349470e+01  3.371332e+01  3.578067e+01  3.460883e+01  3.459970e+01   \n",
       "75%    4.327298e+01  4.326261e+01  4.390462e+01  4.435869e+01  4.429033e+01   \n",
       "max    6.310400e+01  6.422200e+01  6.720000e+01  6.812823e+01  6.927200e+01   \n",
       "\n",
       "               2005          2006          2007          2008          2009  \\\n",
       "count  1.571626e+06  1.570545e+06  1.587869e+06  1.579729e+06  1.573119e+06   \n",
       "mean   3.317612e+01  3.329809e+01  3.287351e+01  3.321420e+01  3.320171e+01   \n",
       "std    1.186311e+01  1.179777e+01  1.205750e+01  1.193083e+01  1.184485e+01   \n",
       "min    6.799689e-02  7.899690e-02  4.599689e-02  1.459969e-01  1.899689e-02   \n",
       "25%    2.221823e+01  2.210608e+01  2.171053e+01  2.209200e+01  2.220668e+01   \n",
       "50%    3.409866e+01  3.417763e+01  3.401879e+01  3.436450e+01  3.401798e+01   \n",
       "75%    4.407848e+01  4.423293e+01  4.395175e+01  4.411874e+01  4.414447e+01   \n",
       "max    6.789000e+01  6.712600e+01  6.914400e+01  6.941600e+01  6.787400e+01   \n",
       "\n",
       "               2010  \n",
       "count  1.581705e+06  \n",
       "mean   3.301338e+01  \n",
       "std    1.192258e+01  \n",
       "min    9.199689e-02  \n",
       "25%    2.207667e+01  \n",
       "50%    3.408301e+01  \n",
       "75%    4.400641e+01  \n",
       "max    6.725800e+01  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_par_means.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: read cleaned excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read new data\n",
    "# read each page from excel file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# find the path of the file called 'cleaned.xlsx'\n",
    "filename='cleaned'\n",
    "Pacific=pd.read_excel(filename+'.xlsx',sheet_name='Pacific')\n",
    "Atlantic=pd.read_excel(filename+'.xlsx',sheet_name='Atlantic')\n",
    "Mediterranean=pd.read_excel(filename+'.xlsx',sheet_name='Mediterranean')\n",
    "Southern=pd.read_excel(filename+'.xlsx',sheet_name='Southern Ocean')\n",
    "Arctic=pd.read_excel(filename+'.xlsx',sheet_name='Arctic')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step four: Melt for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par_melt = df_par_means.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='mld')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step five: Select decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then merge on these columns\n",
    "# assuming df_melted_variable1-5 and ocean_dfs are your dataframes\n",
    "ocean_names = [\"Pacific\", \"Atlantic\", \"Mediterranean\", \"Southern\", \"Arctic\"]\n",
    "# change this to your dataframes\n",
    "df_melted_list = [df_par_melt]\n",
    "ocean_dfs = [Pacific, Atlantic, Mediterranean, Southern, Arctic]\n",
    "\n",
    "\n",
    "for df in df_melted_list + ocean_dfs:\n",
    "    # convert types safely\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
    "    df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "    df['Long'] = pd.to_numeric(df['Long'], errors='coerce')\n",
    "\n",
    "\n",
    "# round lat and long to 0.0001\n",
    "# I believe round is a way of thresholding the data, let me know if there is a better wayof matching \n",
    "# the lat and longs\n",
    "for df in df_melted_list + ocean_dfs:\n",
    "    df['Lat'] = df['Lat'].round(1)\n",
    "    df['Long'] = df['Long'].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common coordinates between Pacific and df_npp: 16\n"
     ]
    }
   ],
   "source": [
    "# check if there is a match\n",
    "\n",
    "# Create a set of tuples for the Pacific dataframe\n",
    "pacific_coords = set(zip(Pacific['Lat'], Pacific['Long']))\n",
    "\n",
    "# Create a set of tuples for the df_npp dataframe\n",
    "npp_coords = set(zip(df_par_melt['Lat'], df_par_melt['Long']))\n",
    "\n",
    "# Find the common coordinates\n",
    "common_coords = pacific_coords.intersection(npp_coords)\n",
    "\n",
    "print(f\"Common coordinates between Pacific and df_npp: {len(common_coords)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step six: Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "merged_dfs = []  # List to store merged dataframes\n",
    "\n",
    "for ocean_name, ocean_df in zip(ocean_names, ocean_dfs):\n",
    "    # merge ocean_df with all dataframes in df_melted_list\n",
    "    for df_melted in df_melted_list:\n",
    "        ocean_df = pd.merge(ocean_df, df_melted, on=['Lat', 'Long', 'Year'], how='left')\n",
    "\n",
    "    # Fill NaN values with the mean of each column\n",
    "    for col in ocean_df.select_dtypes(include=[np.number]).columns:\n",
    "        ocean_df[col] = ocean_df[col].astype(float)\n",
    "        ocean_df[col].fillna(ocean_df[col].mean(), inplace=True)\n",
    "        if ocean_df[col].apply(float.is_integer).all():  # Check if all values are integer\n",
    "            ocean_df[col] = ocean_df[col].astype('Int64')  # Change dtype back to integer\n",
    "\n",
    "    merged_dfs.append(ocean_df)  # Append the merged dataframe to the list\n",
    "\n",
    "with pd.ExcelWriter('merged_par_s.xlsx') as writer:\n",
    "    for ocean_name, merged_df in zip(ocean_names, merged_dfs):\n",
    "        merged_df.to_excel(writer, sheet_name=ocean_name)  # Write each merged dataframe to a different sheet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/par.s.2002.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2003.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2004.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2005.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2006.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2007.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2008.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2009.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2010.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2011.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2012.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2013.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2014.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2015.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2016.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2017.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2018.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2019.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2020.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2021.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2022.tar' already exists. Skipping download.\n",
      "File 'data/par.s.2023.tar' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(url, filename):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# make sure 'data/' directory exists\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')\n",
    "\n",
    "\n",
    "# download the data from MODIS:  http://orca.science.oregonstate.edu/1080.by.2160.monthly.hdf.vgpm.m.chl.m.sst.php\n",
    "\n",
    "new_url = 'http://orca.science.oregonstate.edu/data/1x2/monthly/par.modis.r2022/hdf/par.m.{}.tar'\n",
    "\n",
    "for year in range(2002, 2024):  # 2024 is not included\n",
    "    url = new_url.format(year)\n",
    "    filename = f'data/par.s.{year}.tar'\n",
    "    download_file(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the latitude and longitude arrays\n",
    "lats = np.linspace(90, -90, 1080)\n",
    "lons = np.linspace(-180, 180, 2160)\n",
    "\n",
    "# Create a meshgrid of latitudes and longitudes\n",
    "lon_grid, lat_grid = np.meshgrid(lons, lats, indexing='ij')\n",
    "\n",
    "# Flatten the arrays for dataframe construction\n",
    "lat_flat = lat_grid.flatten()\n",
    "lon_flat = lon_grid.flatten()\n",
    "\n",
    "# Create a multi-index from the latitude and longitude arrays\n",
    "index = pd.MultiIndex.from_arrays([lat_flat, lon_flat], names=['Lat', 'Long'])\n",
    "\n",
    "# Create an empty DataFrame with this index\n",
    "df = pd.DataFrame(index=index)\n",
    "\n",
    "# Loop through each tar file\n",
    "for year in range(1997, 2024):\n",
    "    # Open the tar file\n",
    "    tar_filename = f'data/par.s.{year}.tar'\n",
    "    with tarfile.open(tar_filename, \"r:\") as tar:\n",
    "\n",
    "         # Create an empty DataFrame to store data for this year\n",
    "        temp_df = pd.DataFrame(index=index)\n",
    "\n",
    "        # Loop through each member of the tar file\n",
    "        for member in tar.getmembers():\n",
    "            # If it's a .hdf.gz file\n",
    "            if member.name.endswith('.hdf.gz'):\n",
    "                # Open the .hdf.gz file\n",
    "                f = tar.extractfile(member)\n",
    "                with gzip.open(f, 'rb') as gz:\n",
    "                    # Decompress the .hdf.gz file to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile() as tmp:\n",
    "                        tmp.write(gz.read())\n",
    "                        tmp.seek(0)  # Go back to the start of the file\n",
    "\n",
    "                        # Open the temporary .hdf file\n",
    "                        hdf_file = SD(tmp.name, SDC.READ)\n",
    "\n",
    "                        # Access the 'npp' dataset\n",
    "                        data = hdf_file.select('par')[:]\n",
    "\n",
    "                        # Replace '-9999.0' with NaN\n",
    "                        data[data == -9999.0] = np.nan\n",
    "\n",
    "                        # Flatten the data and add it to the temporary DataFrame\n",
    "                        data_flat = data.flatten()\n",
    "                        temp_df[member.name] = data_flat\n",
    "\n",
    "                        # Close the file\n",
    "                        hdf_file.end()\n",
    "\n",
    "        # After going through all files for the year, calculate the mean for each location (ignoring NaNs)\n",
    "        mean_data = temp_df.mean(axis=1, skipna=True)\n",
    "\n",
    "        # Add this DataFrame to the main DataFrame\n",
    "        df[str(year)] = mean_data\n",
    "\n",
    "# Reset the index of the DataFrame, making 'Lat' and 'Long' normal columns\n",
    "df.reset_index(inplace=True)\n",
    "                      \n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('par_means_m.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.833179</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.666358</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.499537</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.332715</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lat   Long  1997  1998  1999  2000  2001  2002  2003  2004  ...  \\\n",
       "0  90.000000 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "1  89.833179 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2  89.666358 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3  89.499537 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4  89.332715 -180.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "   2014  2015  2016  2017  2018  2019  2020  2021  2022  2023  \n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_par_means_m = pd.read_csv('par_means_m.csv')\n",
    "df_par_means_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.332800e+06</td>\n",
       "      <td>2.332800e+06</td>\n",
       "      <td>1.328358e+06</td>\n",
       "      <td>1.431556e+06</td>\n",
       "      <td>1.434942e+06</td>\n",
       "      <td>1.433280e+06</td>\n",
       "      <td>1.428070e+06</td>\n",
       "      <td>1.525700e+06</td>\n",
       "      <td>1.559677e+06</td>\n",
       "      <td>1.564153e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.574863e+06</td>\n",
       "      <td>1.597800e+06</td>\n",
       "      <td>1.618205e+06</td>\n",
       "      <td>1.605778e+06</td>\n",
       "      <td>1.609688e+06</td>\n",
       "      <td>1.616116e+06</td>\n",
       "      <td>1.629161e+06</td>\n",
       "      <td>1.616048e+06</td>\n",
       "      <td>1.615011e+06</td>\n",
       "      <td>1.349397e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.347462e-14</td>\n",
       "      <td>5.988442e-16</td>\n",
       "      <td>3.648680e+01</td>\n",
       "      <td>3.293971e+01</td>\n",
       "      <td>3.286141e+01</td>\n",
       "      <td>3.272164e+01</td>\n",
       "      <td>3.257054e+01</td>\n",
       "      <td>3.404737e+01</td>\n",
       "      <td>3.359226e+01</td>\n",
       "      <td>3.340154e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.313329e+01</td>\n",
       "      <td>3.333278e+01</td>\n",
       "      <td>3.315810e+01</td>\n",
       "      <td>3.326634e+01</td>\n",
       "      <td>3.323867e+01</td>\n",
       "      <td>3.325779e+01</td>\n",
       "      <td>3.290469e+01</td>\n",
       "      <td>3.324599e+01</td>\n",
       "      <td>3.346675e+01</td>\n",
       "      <td>3.945527e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.200967e+01</td>\n",
       "      <td>1.039712e+02</td>\n",
       "      <td>1.437401e+01</td>\n",
       "      <td>1.081675e+01</td>\n",
       "      <td>1.097235e+01</td>\n",
       "      <td>1.110469e+01</td>\n",
       "      <td>1.137511e+01</td>\n",
       "      <td>1.146245e+01</td>\n",
       "      <td>1.167125e+01</td>\n",
       "      <td>1.173296e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.198258e+01</td>\n",
       "      <td>1.169108e+01</td>\n",
       "      <td>1.174541e+01</td>\n",
       "      <td>1.175228e+01</td>\n",
       "      <td>1.176685e+01</td>\n",
       "      <td>1.176853e+01</td>\n",
       "      <td>1.203324e+01</td>\n",
       "      <td>1.172796e+01</td>\n",
       "      <td>1.144901e+01</td>\n",
       "      <td>1.570208e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.000000e+01</td>\n",
       "      <td>-1.800000e+02</td>\n",
       "      <td>1.999000e-03</td>\n",
       "      <td>1.199300e-02</td>\n",
       "      <td>2.119980e-01</td>\n",
       "      <td>2.999900e-02</td>\n",
       "      <td>1.199300e-02</td>\n",
       "      <td>3.266356e-02</td>\n",
       "      <td>5.799689e-02</td>\n",
       "      <td>7.399689e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399689e-02</td>\n",
       "      <td>2.239969e-01</td>\n",
       "      <td>2.199689e-02</td>\n",
       "      <td>4.199689e-02</td>\n",
       "      <td>5.399689e-02</td>\n",
       "      <td>1.229969e-01</td>\n",
       "      <td>7.599689e-02</td>\n",
       "      <td>2.599689e-02</td>\n",
       "      <td>2.799689e-02</td>\n",
       "      <td>1.899689e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.500000e+01</td>\n",
       "      <td>-9.000000e+01</td>\n",
       "      <td>2.955235e+01</td>\n",
       "      <td>2.242377e+01</td>\n",
       "      <td>2.230057e+01</td>\n",
       "      <td>2.197562e+01</td>\n",
       "      <td>2.159606e+01</td>\n",
       "      <td>2.379849e+01</td>\n",
       "      <td>2.268313e+01</td>\n",
       "      <td>2.230756e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.216235e+01</td>\n",
       "      <td>2.247413e+01</td>\n",
       "      <td>2.246878e+01</td>\n",
       "      <td>2.246191e+01</td>\n",
       "      <td>2.224007e+01</td>\n",
       "      <td>2.242867e+01</td>\n",
       "      <td>2.205770e+01</td>\n",
       "      <td>2.243214e+01</td>\n",
       "      <td>2.318759e+01</td>\n",
       "      <td>3.223234e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.105427e-15</td>\n",
       "      <td>-1.425249e-14</td>\n",
       "      <td>3.971651e+01</td>\n",
       "      <td>3.352026e+01</td>\n",
       "      <td>3.345121e+01</td>\n",
       "      <td>3.349470e+01</td>\n",
       "      <td>3.371332e+01</td>\n",
       "      <td>3.578067e+01</td>\n",
       "      <td>3.460883e+01</td>\n",
       "      <td>3.459970e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.399547e+01</td>\n",
       "      <td>3.414306e+01</td>\n",
       "      <td>3.373656e+01</td>\n",
       "      <td>3.412461e+01</td>\n",
       "      <td>3.418643e+01</td>\n",
       "      <td>3.399836e+01</td>\n",
       "      <td>3.382202e+01</td>\n",
       "      <td>3.405156e+01</td>\n",
       "      <td>3.416727e+01</td>\n",
       "      <td>4.142003e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>4.783021e+01</td>\n",
       "      <td>4.351616e+01</td>\n",
       "      <td>4.318112e+01</td>\n",
       "      <td>4.327298e+01</td>\n",
       "      <td>4.326261e+01</td>\n",
       "      <td>4.390462e+01</td>\n",
       "      <td>4.435869e+01</td>\n",
       "      <td>4.429033e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.426788e+01</td>\n",
       "      <td>4.431431e+01</td>\n",
       "      <td>4.426306e+01</td>\n",
       "      <td>4.405948e+01</td>\n",
       "      <td>4.410228e+01</td>\n",
       "      <td>4.428291e+01</td>\n",
       "      <td>4.393111e+01</td>\n",
       "      <td>4.400210e+01</td>\n",
       "      <td>4.387196e+01</td>\n",
       "      <td>5.175900e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>6.624800e+01</td>\n",
       "      <td>6.409000e+01</td>\n",
       "      <td>6.307400e+01</td>\n",
       "      <td>6.310400e+01</td>\n",
       "      <td>6.422200e+01</td>\n",
       "      <td>6.720000e+01</td>\n",
       "      <td>6.812823e+01</td>\n",
       "      <td>6.927200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.885600e+01</td>\n",
       "      <td>6.776150e+01</td>\n",
       "      <td>6.554400e+01</td>\n",
       "      <td>6.914800e+01</td>\n",
       "      <td>6.658200e+01</td>\n",
       "      <td>6.887600e+01</td>\n",
       "      <td>6.776000e+01</td>\n",
       "      <td>6.863000e+01</td>\n",
       "      <td>6.780400e+01</td>\n",
       "      <td>6.852000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Lat          Long          1997          1998          1999  \\\n",
       "count  2.332800e+06  2.332800e+06  1.328358e+06  1.431556e+06  1.434942e+06   \n",
       "mean   1.347462e-14  5.988442e-16  3.648680e+01  3.293971e+01  3.286141e+01   \n",
       "std    5.200967e+01  1.039712e+02  1.437401e+01  1.081675e+01  1.097235e+01   \n",
       "min   -9.000000e+01 -1.800000e+02  1.999000e-03  1.199300e-02  2.119980e-01   \n",
       "25%   -4.500000e+01 -9.000000e+01  2.955235e+01  2.242377e+01  2.230057e+01   \n",
       "50%    7.105427e-15 -1.425249e-14  3.971651e+01  3.352026e+01  3.345121e+01   \n",
       "75%    4.500000e+01  9.000000e+01  4.783021e+01  4.351616e+01  4.318112e+01   \n",
       "max    9.000000e+01  1.800000e+02  6.624800e+01  6.409000e+01  6.307400e+01   \n",
       "\n",
       "               2000          2001          2002          2003          2004  \\\n",
       "count  1.433280e+06  1.428070e+06  1.525700e+06  1.559677e+06  1.564153e+06   \n",
       "mean   3.272164e+01  3.257054e+01  3.404737e+01  3.359226e+01  3.340154e+01   \n",
       "std    1.110469e+01  1.137511e+01  1.146245e+01  1.167125e+01  1.173296e+01   \n",
       "min    2.999900e-02  1.199300e-02  3.266356e-02  5.799689e-02  7.399689e-02   \n",
       "25%    2.197562e+01  2.159606e+01  2.379849e+01  2.268313e+01  2.230756e+01   \n",
       "50%    3.349470e+01  3.371332e+01  3.578067e+01  3.460883e+01  3.459970e+01   \n",
       "75%    4.327298e+01  4.326261e+01  4.390462e+01  4.435869e+01  4.429033e+01   \n",
       "max    6.310400e+01  6.422200e+01  6.720000e+01  6.812823e+01  6.927200e+01   \n",
       "\n",
       "       ...          2014          2015          2016          2017  \\\n",
       "count  ...  1.574863e+06  1.597800e+06  1.618205e+06  1.605778e+06   \n",
       "mean   ...  3.313329e+01  3.333278e+01  3.315810e+01  3.326634e+01   \n",
       "std    ...  1.198258e+01  1.169108e+01  1.174541e+01  1.175228e+01   \n",
       "min    ...  1.399689e-02  2.239969e-01  2.199689e-02  4.199689e-02   \n",
       "25%    ...  2.216235e+01  2.247413e+01  2.246878e+01  2.246191e+01   \n",
       "50%    ...  3.399547e+01  3.414306e+01  3.373656e+01  3.412461e+01   \n",
       "75%    ...  4.426788e+01  4.431431e+01  4.426306e+01  4.405948e+01   \n",
       "max    ...  6.885600e+01  6.776150e+01  6.554400e+01  6.914800e+01   \n",
       "\n",
       "               2018          2019          2020          2021          2022  \\\n",
       "count  1.609688e+06  1.616116e+06  1.629161e+06  1.616048e+06  1.615011e+06   \n",
       "mean   3.323867e+01  3.325779e+01  3.290469e+01  3.324599e+01  3.346675e+01   \n",
       "std    1.176685e+01  1.176853e+01  1.203324e+01  1.172796e+01  1.144901e+01   \n",
       "min    5.399689e-02  1.229969e-01  7.599689e-02  2.599689e-02  2.799689e-02   \n",
       "25%    2.224007e+01  2.242867e+01  2.205770e+01  2.243214e+01  2.318759e+01   \n",
       "50%    3.418643e+01  3.399836e+01  3.382202e+01  3.405156e+01  3.416727e+01   \n",
       "75%    4.410228e+01  4.428291e+01  4.393111e+01  4.400210e+01  4.387196e+01   \n",
       "max    6.658200e+01  6.887600e+01  6.776000e+01  6.863000e+01  6.780400e+01   \n",
       "\n",
       "               2023  \n",
       "count  1.349397e+06  \n",
       "mean   3.945527e+01  \n",
       "std    1.570208e+01  \n",
       "min    1.899689e-02  \n",
       "25%    3.223234e+01  \n",
       "50%    4.142003e+01  \n",
       "75%    5.175900e+01  \n",
       "max    6.852000e+01  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_par_means_m.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read new data\n",
    "# read each page from excel file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# find the path of the file called 'cleaned.xlsx'\n",
    "filename='cleaned'\n",
    "Pacific=pd.read_excel(filename+'.xlsx',sheet_name='Pacific')\n",
    "Atlantic=pd.read_excel(filename+'.xlsx',sheet_name='Atlantic')\n",
    "Mediterranean=pd.read_excel(filename+'.xlsx',sheet_name='Mediterranean')\n",
    "Southern=pd.read_excel(filename+'.xlsx',sheet_name='Southern Ocean')\n",
    "Arctic=pd.read_excel(filename+'.xlsx',sheet_name='Arctic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_par_melt_m = df_par_means_m.melt(id_vars=['Lat', 'Long'], var_name='Year', value_name='mld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then merge on these columns\n",
    "# assuming df_melted_variable1-5 and ocean_dfs are your dataframes\n",
    "ocean_names = [\"Pacific\", \"Atlantic\", \"Mediterranean\", \"Southern\", \"Arctic\"]\n",
    "# change this to your dataframes\n",
    "df_melted_list = [df_par_melt_m]\n",
    "ocean_dfs = [Pacific, Atlantic, Mediterranean, Southern, Arctic]\n",
    "\n",
    "\n",
    "for df in df_melted_list + ocean_dfs:\n",
    "    # convert types safely\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
    "    df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "    df['Long'] = pd.to_numeric(df['Long'], errors='coerce')\n",
    "\n",
    "\n",
    "# round lat and long to 0.0001\n",
    "# I believe round is a way of thresholding the data, let me know if there is a better wayof matching \n",
    "# the lat and longs\n",
    "for df in df_melted_list + ocean_dfs:\n",
    "    df['Lat'] = df['Lat'].round(1)\n",
    "    df['Long'] = df['Long'].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common coordinates between Pacific and df_npp: 16\n"
     ]
    }
   ],
   "source": [
    "# check if there is a match\n",
    "\n",
    "# Create a set of tuples for the Pacific dataframe\n",
    "pacific_coords = set(zip(Pacific['Lat'], Pacific['Long']))\n",
    "\n",
    "# Create a set of tuples for the df_npp dataframe\n",
    "npp_coords = set(zip(df_par_melt_m['Lat'], df_par_melt_m['Long']))\n",
    "\n",
    "# Find the common coordinates\n",
    "common_coords = pacific_coords.intersection(npp_coords)\n",
    "\n",
    "print(f\"Common coordinates between Pacific and df_npp: {len(common_coords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "merged_dfs = []  # List to store merged dataframes\n",
    "\n",
    "for ocean_name, ocean_df in zip(ocean_names, ocean_dfs):\n",
    "    # merge ocean_df with all dataframes in df_melted_list\n",
    "    for df_melted in df_melted_list:\n",
    "        ocean_df = pd.merge(ocean_df, df_melted, on=['Lat', 'Long', 'Year'], how='left')\n",
    "\n",
    "    # Fill NaN values with the mean of each column\n",
    "    for col in ocean_df.select_dtypes(include=[np.number]).columns:\n",
    "        ocean_df[col] = ocean_df[col].astype(float)\n",
    "        ocean_df[col].fillna(ocean_df[col].mean(), inplace=True)\n",
    "        if ocean_df[col].apply(float.is_integer).all():  # Check if all values are integer\n",
    "            ocean_df[col] = ocean_df[col].astype('Int64')  # Change dtype back to integer\n",
    "\n",
    "    merged_dfs.append(ocean_df)  # Append the merged dataframe to the list\n",
    "\n",
    "with pd.ExcelWriter('merged_par_m.xlsx') as writer:\n",
    "    for ocean_name, merged_df in zip(ocean_names, merged_dfs):\n",
    "        merged_df.to_excel(writer, sheet_name=ocean_name)  # Write each merged dataframe to a different sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
